<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Minicurso de verano de Aprendizaje Máquina</title>
  <meta name="description" content="Minicurso de Aprendizaje Máquina, ITAM 2018.">
  <meta name="generator" content="bookdown 0.7.12 and GitBook 2.6.7">

  <meta property="og:title" content="Minicurso de verano de Aprendizaje Máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-verano-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Minicurso de verano de Aprendizaje Máquina" />
  
  <meta name="twitter:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-06-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="regresion-lineal.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Minicurso aprendizaje máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias"><i class="fa fa-check"></i>Referencias</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i><b>1.2.1</b> Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#cuantificacion-de-error-o-precision"><i class="fa fa-check"></i><b>1.4</b> Cuantificación de error o precisión</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.5</b> Tarea de aprendizaje supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#observaciones"><i class="fa fa-check"></i>Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#por-que-tenemos-errores"><i class="fa fa-check"></i><b>1.6</b> ¿Por qué tenemos errores?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion-lineal.html"><a href="regresion-lineal.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#introduccion-1"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion-lineal.html"><a href="regresion-lineal.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion-lineal.html"><a href="regresion-lineal.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion-lineal.html"><a href="regresion-lineal.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.7</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.7.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.7.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion-lineal.html"><a href="regresion-lineal.html#ejercicio-1"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regresion-logistica.html"><a href="regresion-logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-2"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-2"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="regresion-logistica.html"><a href="regresion-logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="regresion-logistica.html"><a href="regresion-logistica.html#observaciones-adicionales"><i class="fa fa-check"></i><b>3.6</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i>Ejercicio: datos de diabetes</a></li>
<li class="chapter" data-level="3.7" data-path="regresion-logistica.html"><a href="regresion-logistica.html#mas-sobre-problemas-de-clasificacion"><i class="fa fa-check"></i><b>3.7</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="3.7.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>3.7.1</b> Análisis de error para clasificadores binarios</a></li>
<li class="chapter" data-level="3.7.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>3.7.2</b> Regresión logística para problemas de más de 2 clases</a></li>
<li class="chapter" data-level="3.7.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>3.7.3</b> Regresión logística multinomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html"><i class="fa fa-check"></i><b>4</b> Regresión regularizada</a><ul>
<li class="chapter" data-level="4.0.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>4.0.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="4.0.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>4.0.2</b> Reduciendo varianza de los coeficientes</a></li>
<li class="chapter" data-level="4.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-ridge"><i class="fa fa-check"></i><b>4.1</b> Regularización ridge</a><ul>
<li class="chapter" data-level="4.1.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>4.1.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>4.2</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="4.2.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#validacion-cruzada"><i class="fa fa-check"></i><b>4.2.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-lasso"><i class="fa fa-check"></i><b>4.3</b> Regularización lasso</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html"><i class="fa fa-check"></i><b>5</b> Descenso estocástico</a><ul>
<li class="chapter" data-level="5.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>5.1</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="5.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>5.2</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="5.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.3</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>5.4</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="5.4.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.4.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#momento"><i class="fa fa-check"></i><b>5.4.2</b> Momento</a></li>
<li class="chapter" data-level="5.4.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#otras-variaciones"><i class="fa fa-check"></i><b>5.4.3</b> Otras variaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>6</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="6.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>6.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="6.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>6.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="6.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>6.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="6.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>6.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="6.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>6.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="6.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>6.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="6.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>6.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="6.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>6.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>7</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="7.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>7.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="7.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>7.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="7.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>7.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="7.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>7.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="7.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>7.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="7.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>7.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="7.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>7.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="7.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>7.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="7.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>7.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="7.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>7.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="7.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>7.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>7.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="7.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-28"><i class="fa fa-check"></i><b>7.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>7.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>7.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="7.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>7.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="7.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-29"><i class="fa fa-check"></i><b>7.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="7.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>7.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="7.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>7.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="7.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>7.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="7.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles-aleatorios"><i class="fa fa-check"></i><b>7.3.6</b> Ventajas y desventajas de árboles aleatorios</a></li>
<li class="chapter" data-level="7.3.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tarea-para-23-de-octubre"><i class="fa fa-check"></i><b>7.3.7</b> Tarea (para 23 de octubre)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>8</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="8.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>8.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="8.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>8.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="8.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>8.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="8.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>8.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="8.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>8.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-32"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="8.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>8.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="8.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>8.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>8.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-5"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>8.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="8.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>8.8</b> Resumen</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Minicurso de verano de Aprendizaje Máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduccion" class="section level1">
<h1><span class="header-section-number">Chapter 1</span> Introducción</h1>
<div id="que-es-aprendizaje-de-maquina-machine-learning" class="section level2">
<h2><span class="header-section-number">1.1</span> ¿Qué es aprendizaje de máquina (machine learning)?</h2>
<p>Métodos <strong>computacionales</strong> para <strong>aprender de datos</strong> con el fin
de producir reglas para
mejorar el <strong>desempeño</strong> en alguna tarea o toma de decisión.</p>
<p>En este curso nos enfocamos en las tareas de aprendizaje supervisado,
donde intentamos
predecir o estimar una variable respuesta a partir de datos de entrada.</p>
<div id="ejemplos-de-tareas-de-aprendizaje" class="section level4 unnumbered">
<h4>Ejemplos de tareas de aprendizaje:</h4>
<ul>
<li>Predecir si un cliente de tarjeta de crédito va a caer en impago en los próximos
tres meses.</li>
<li>Reconocer palabras escritas a mano (OCR).</li>
<li>Detectar llamados de ballenas en grabaciones de boyas.</li>
<li>Estimar el ingreso mensual de un hogar a partir de las características
de la vivienda, posesiones y equipamiento y localización geográfica.</li>
<li>Dividir a los clientes de Netflix según sus gustos.</li>
<li>Recomendar artículos a clientes de un programa de lealtad o servicio online.</li>
</ul>
<p>Las razones usuales para intentar resolver estos problemas <strong>computacionalmente</strong>
son diversas:</p>
<ul>
<li>Quisiéramos obtener una respuesta barata, rápida, <strong>automatizada</strong>, y
con suficiente precisión.
Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por
personas, pero eso es lento y costoso. Igual oír cada segundo de grabación
de las boyas para saber si hay ballenas o no. Hacer mediciones directas
del ingreso de un hogar requiere mucho tiempo y esfuerzo.</li>
<li>Quisiéramos <strong>superar el desempeño actual</strong> de los expertos o de reglas simples utilizando
datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante,
puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales
o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.</li>
<li>Queremos <strong>entender de manera más completa y sistemática</strong> el comportamiento de un fenómeno,
identificando variables o patrones importantes.</li>
</ul>
<p>Es posible aproximarse a todos estos problemas usando reglas (por ejemplo,
si los pixeles del centro de la imagen están vacíos, entonces es un cero,
si el crédito total es mayor al 50% del ingreso anual, declinar el préstamo, etc)
Las razones para intentar usar <strong>aprendizaje</strong> para producir reglas en lugar
de intentar construir estas reglas directamente son, por ejemplo:</p>
<ul>
<li>Cuando conjuntos de reglas creadas a mano se desempeñan mal (por ejemplo, para
otorgar créditos, reconocer caracteres, etc.)</li>
<li>Reglas creadas a mano pueden ser difíciles de mantener (por ejemplo, un corrector
ortográfico.)</li>
</ul>
</div>
<div id="ejemplo-reconocimiento-de-digitos-escritos-a-mano" class="section level4 unnumbered">
<h4>Ejemplo: reconocimiento de dígitos escritos a mano</h4>
<p>¿Cómo reconocer los siguientes dígitos de manera automática?</p>
<p>En los datos tenemos los valores de cada pixel (los caracteres son
imagenes de 16x16 pixeles), y una <em>etiqueta</em> asociada, que es el número
que la imagen representa. Podemos ver las imágenes y las etiquetas:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
zip_train &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&#39;datos/zip-train.csv&#39;</span>)
muestra_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">sample_n</span>(zip_train, <span class="dv">10</span>)
<span class="kw">graficar_digitos</span>(muestra_<span class="dv">1</span>)</code></pre>
<p><img src="01-intro_files/figure-html/grafdigitos-1.png" width="400px" /></p>
<pre class="sourceCode r"><code class="sourceCode r">muestra_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">sample_n</span>(zip_train, <span class="dv">10</span>) 
<span class="kw">graficar_digitos</span>(muestra_<span class="dv">2</span>)</code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-2-1.png" width="400px" /></p>
<p>Los 16x16=256 están escritos acomodando las filas de la imagen en
vector de 256 valores (cada renglón de <code>zip_train</code>). Un dígito entonces
se representa como in vector de 256 números:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(zip_train)</code></pre>
<pre><code>## [1] 7291  257</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">as.numeric</span>(zip_train[<span class="dv">1</span>,])</code></pre>
<pre><code>##   [1]  6.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.631  0.862
##  [11] -0.167 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
##  [21] -1.000 -1.000 -1.000 -0.992  0.297  1.000  0.307 -1.000 -1.000 -1.000
##  [31] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.410
##  [41]  1.000  0.986 -0.565 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
##  [51] -1.000 -1.000 -1.000 -1.000 -0.683  0.825  1.000  0.562 -1.000 -1.000
##  [61] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.938
##  [71]  0.540  1.000  0.778 -0.715 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
##  [81] -1.000 -1.000 -1.000 -1.000 -1.000  0.100  1.000  0.922 -0.439 -1.000
##  [91] -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000
## [101] -0.257  0.950  1.000 -0.162 -1.000 -1.000 -1.000 -0.987 -0.714 -0.832
## [111] -1.000 -1.000 -1.000 -1.000 -1.000 -0.797  0.909  1.000  0.300 -0.961
## [121] -1.000 -1.000 -0.550  0.485  0.996  0.867  0.092 -1.000 -1.000 -1.000
## [131] -1.000  0.278  1.000  0.877 -0.824 -1.000 -0.905  0.145  0.977  1.000
## [141]  1.000  1.000  0.990 -0.745 -1.000 -1.000 -0.950  0.847  1.000  0.327
## [151] -1.000 -1.000  0.355  1.000  0.655 -0.109 -0.185  1.000  0.988 -0.723
## [161] -1.000 -1.000 -0.630  1.000  1.000  0.068 -0.925  0.113  0.960  0.308
## [171] -0.884 -1.000 -0.075  1.000  0.641 -0.995 -1.000 -1.000 -0.677  1.000
## [181]  1.000  0.753  0.341  1.000  0.707 -0.942 -1.000 -1.000  0.545  1.000
## [191]  0.027 -1.000 -1.000 -1.000 -0.903  0.792  1.000  1.000  1.000  1.000
## [201]  0.536  0.184  0.812  0.837  0.978  0.864 -0.630 -1.000 -1.000 -1.000
## [211] -1.000 -0.452  0.828  1.000  1.000  1.000  1.000  1.000  1.000  1.000
## [221]  1.000  0.135 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -0.483  0.813
## [231]  1.000  1.000  1.000  1.000  1.000  1.000  0.219 -0.943 -1.000 -1.000
## [241] -1.000 -1.000 -1.000 -1.000 -1.000 -0.974 -0.429  0.304  0.823  1.000
## [251]  0.482 -0.474 -0.991 -1.000 -1.000 -1.000 -1.000</code></pre>
<p>¿Cómo combinamos esta información en un cálculo para reconocer el dígito
computacionalmente? Un enfoque más utilizado anteriormente para resolver este tipo de problemas consistía en procesar estas imágenes con filtros hechos a mano (por ejemplo,
calcular cuántos pixeles están prendidos, si existen ciertas curvas o trazos)
para después construir reglas para determinar cada dígito. Actualmente,
el enfoque más exitoso es utilizar métodos de aprendizaje que aprendan
automáticamente esos filtros y esas reglas basadas en filtros (redes convolucionales).</p>
</div>
<div id="ejemplo-predecir-ingreso-trimestral" class="section level4 unnumbered">
<h4>Ejemplo: predecir ingreso trimestral</h4>
<p>Consideramos la medición de ingreso total trimestral para una
muestra de hogares de la encuesta de ENIGH. Cada una de estas mediciones
es muy costosa en tiempo y dinero.</p>
<pre class="sourceCode r"><code class="sourceCode r">dat_ingreso &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="dt">file =</span> <span class="st">&#39;datos/enigh-ejemplo.csv&#39;</span>)
<span class="kw">head</span>(dat_ingreso) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(TAM_HOG, INGCOR, NOM_ENT_<span class="dv">1</span>, FOCOS, 
           PISOS, marginación, tamaño_localidad)  <span class="op">%&gt;%</span>
<span class="st">    </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">TAM_HOG</th>
<th align="right">INGCOR</th>
<th align="left">NOM_ENT_1</th>
<th align="right">FOCOS</th>
<th align="right">PISOS</th>
<th align="left">marginación</th>
<th align="left">tamaño_localidad</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">4</td>
<td align="right">30238.13</td>
<td align="left">Jalisco</td>
<td align="right">11</td>
<td align="right">3</td>
<td align="left">Muy bajo</td>
<td align="left">De 15 mil a 100 mil</td>
</tr>
<tr class="even">
<td align="right">3</td>
<td align="right">61147.41</td>
<td align="left">México</td>
<td align="right">10</td>
<td align="right">2</td>
<td align="left">Bajo</td>
<td align="left">De 15 mil a 100 mil</td>
</tr>
<tr class="odd">
<td align="right">2</td>
<td align="right">6170.21</td>
<td align="left">Puebla</td>
<td align="right">1</td>
<td align="right">1</td>
<td align="left">Alto</td>
<td align="left">De 2500 a 15 mil</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">14639.79</td>
<td align="left">Distrito Federal</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="left">Muy bajo</td>
<td align="left">100 mil o más</td>
</tr>
<tr class="odd">
<td align="right">1</td>
<td align="right">40638.35</td>
<td align="left">Chihuahua</td>
<td align="right">8</td>
<td align="right">3</td>
<td align="left">Muy bajo</td>
<td align="left">De 15 mil a 100 mil</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">21172.35</td>
<td align="left">Baja California</td>
<td align="right">4</td>
<td align="right">2</td>
<td align="left">Muy bajo</td>
<td align="left">100 mil o más</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_ingreso, <span class="kw">aes</span>(<span class="dt">x=</span>INGTOT)) <span class="op">+</span><span class="st">  </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">bins =</span> <span class="dv">100</span>) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_log10</span>()</code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-5-1.png" width="480" /></p>
<p>Pero quizá podemos usar otras variables más fácilmente medibles
para predecir el ingreso de un hogar. Por ejemplo, si consideramos el número
de focos en la vivienda:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_ingreso, <span class="kw">aes</span>(<span class="dt">x =</span> FOCOS, <span class="dt">y =</span> INGTOT)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_log10</span>() <span class="op">+</span><span class="st"> </span><span class="kw">xlim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">50</span>))</code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-6-1.png" width="480" /></p>
<p>O el tamaño de la localidad:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat_ingreso, <span class="kw">aes</span>(<span class="dt">x =</span> tamaño_localidad, <span class="dt">y =</span> INGTOT)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">scale_y_log10</span>() </code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-7-1.png" width="480" /></p>
<p>En algunas encuestas se pregunta directamente el ingreso mensual del hogar. La
respuesta es generalmente una mala estimación del verdadero ingreso, por lo que
actualmente se prefiere utilizar aprendizaje para estimar a partir de otras
variables que son más fielmente reportadas por encuestados (años de estudio,
ocupación, número de focos en el hogar, etc.)</p>
</div>
</div>
<div id="aprendizaje-supervisado" class="section level2">
<h2><span class="header-section-number">1.2</span> Aprendizaje Supervisado</h2>
<p>Por el momento nos concentramos en problemas supervisados de regresión, es
decir predicción de variables numéricas.</p>
<p>¿Cómo entendemos el problema de predicción?</p>
<div id="proceso-generador-de-datos-modelo-teorico" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Proceso generador de datos (modelo teórico)</h3>
<p>Para entender lo que estamos intentando hacer, pensaremos en términos
de <strong>modelos probabilísticos que generan los datos</strong>. La idea es que
estos representan los procesos que generan los datos o las observaciones.</p>
<p>Si <span class="math inline">\(Y\)</span> es la respuesta
que queremos predecir, y <span class="math inline">\(X\)</span> es una entrada que queremos usar para predecir
<span class="math inline">\(Y\)</span>,<br />
consideramos que las variables aleatorias <span class="math inline">\(Y\)</span> y <span class="math inline">\(X\)</span> están relacionadas como sigue:
<span class="math display">\[Y=f(X)+\epsilon,\]</span>
donde <span class="math inline">\(\epsilon\)</span> es una término de error aleatorio que no depende de <span class="math inline">\(X\)</span>, y
que tiene valor esperado <span class="math inline">\(\textrm{E}(\epsilon)=0\)</span>.</p>
<ul>
<li><span class="math inline">\(f\)</span> expresa la relación sistemática que hay entre <span class="math inline">\(Y\)</span> y <span class="math inline">\(X\)</span>: para cada valor
posible de <span class="math inline">\(X\)</span>, la <strong>contribución</strong> de <span class="math inline">\(X\)</span> a <span class="math inline">\(Y\)</span> es <span class="math inline">\(f(X)\)</span>.</li>
<li>Pero <span class="math inline">\(X\)</span> <strong>no determina</strong> a <span class="math inline">\(Y\)</span>, como en el ejemplo anterior de rendimiento
de coches.
Entonces agregamos una error aleatorio <span class="math inline">\(\epsilon\)</span>, con media cero (si la media
no es cero podemos agregar una constante a <span class="math inline">\(f\)</span>), que no contiene información
acerca de <span class="math inline">\(X\)</span> (independiente de <span class="math inline">\(X\)</span>).</li>
<li><span class="math inline">\(\epsilon\)</span> representa, por ejemplo, el efecto de variables que no hemos
medido o procesos aleatorios que determinan la respuesta.</li>
<li>La función <span class="math inline">\(f\)</span> típicamente es complicada y desconocida.</li>
</ul>
<div id="ejemplo" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Vamos a usar simulación para entender estas ideas: supongamos que <span class="math inline">\(X\)</span>
es el número de años de estudio de una persona y <span class="math inline">\(Y\)</span> es su ingreso mensual.
En primer lugar, estas son el número de años de estudio de 8 personas:</p>
<pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">7</span>,<span class="dv">10</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">9</span>,<span class="dv">13</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">17</span>,<span class="dv">18</span>,<span class="dv">1</span>,<span class="dv">2</span>)</code></pre>
<p>Ahora <em>supondremos</em> que la dependencia de Y de X está dada por
<span class="math inline">\(Y=f(X)+\epsilon\)</span> por una función <span class="math inline">\(f\)</span> que no conocemos (esta función está
determinada por el fenómeno)</p>
<pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="cf">function</span>(x){
  <span class="kw">ifelse</span>(x <span class="op">&lt;</span><span class="st"> </span><span class="dv">10</span>, <span class="dv">1000</span><span class="op">*</span><span class="kw">sqrt</span>(x), <span class="dv">1000</span><span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">10</span>))
}</code></pre>
<p>El ingreso no se determina
únicamente por número de años de estudio. Suponemos entonces que hay algunas
variables
adicionales que perturban los niveles de <span class="math inline">\(f(X)\)</span> por una cantidad aleatoria. Los
valores que observamos de <span class="math inline">\(Y\)</span> están dados entonces por <span class="math inline">\(Y=f(X)+\epsilon\)</span>.</p>
<p>Entonces podríamos obtener, por ejemplo:</p>
<pre class="sourceCode r"><code class="sourceCode r">x_g &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">20</span>,<span class="fl">0.5</span>)
y_g &lt;-<span class="st"> </span><span class="kw">f</span>(x_g)
dat_g &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x_g, <span class="dt">y =</span> y_g)
<span class="kw">set.seed</span>(<span class="dv">281</span>)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>error
datos &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)
datos<span class="op">$</span>y_media &lt;-<span class="st"> </span><span class="kw">f</span>(datos<span class="op">$</span>x)
<span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">data=</span>dat_g, <span class="dt">colour =</span> <span class="st">&#39;gray&#39;</span>, <span class="dt">size =</span> <span class="fl">1.1</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">xend =</span> x, <span class="dt">y =</span> y, <span class="dt">yend =</span> y_media), <span class="dt">col=</span><span class="st">&#39;red&#39;</span>)</code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-10-1.png" width="480" /></p>
<p>En problemas de aprendizaje nunca conocemos esta <span class="math inline">\(f\)</span> verdadera,
aunque quizá sabemos algo acerca de sus propiedades (por ejemplo, continua, de
variación suave). Lo que tenemos son los datos, que también podrían haber resultado
en (para otra muestra de personas, por ejemplo):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">28015</span>)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>error
datos &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)
<span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() </code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-11-1.png" width="480" /></p>
<p>La siguiente observación nos da una idea de lo que intentamos hacer, aunque
todavía es vaga y requiere refinamiento:</p>
<div class="comentario">
<p>
Bajo los supuestos del modelo <span class="math inline"><span class="math inline">\(Y=f(X)+\epsilon\)</span></span>, <strong>aprender de los datos</strong> significa intentar recuperar o estimar la forma de la función <span class="math inline"><span class="math inline">\(f\)</span></span> que no conocemos. <span class="math inline"><span class="math inline">\(f\)</span></span> representa la relación sistemática entre <span class="math inline"><span class="math inline">\(Y\)</span></span> y <span class="math inline"><span class="math inline">\(X\)</span></span>.
</p>
</div>
<p>¿Qué tan bien podemos estimar esa <span class="math inline">\(f\)</span> que no conocemos, con
los datos disponibles? ¿Qué significa <em>estimar bien</em>?
Incluso este ejemplo tan simple muestra las dificultades que vamos a
enfrentar, y la importancia de determinar con cuidado qué tanta información
tenemos, y qué tan buenas pueden ser nuestras predicciones.</p>
</div>
</div>
</div>
<div id="predicciones" class="section level2">
<h2><span class="header-section-number">1.3</span> Predicciones</h2>
<p>La idea es entonces producir una estimación de <span class="math inline">\(f\)</span> que nos permita hacer predicciones.</p>
<p>Si denotamos por <span class="math inline">\(\hat{f}\)</span> a una estimación de <span class="math inline">\(f\)</span> construida a partir de los datos,
podemos hacer predicciones aplicando <span class="math inline">\(\hat{f}\)</span> a valores de <span class="math inline">\(X\)</span>.
La predicción de Y la denotamos por <span class="math inline">\(\hat{Y}\)</span>, y <span class="math display">\[\hat{Y}=\hat{f}(X).\]</span>
El error de predicción (residual) está dado por el valor observado menos la predicción:
<span class="math display">\[Y-\hat{Y}.\]</span></p>
<p>En nuestro ejemplo anterior, podríamos construir, por ejemplo, una recta
ajustada por mínimos cuadrados:</p>
<pre class="sourceCode r"><code class="sourceCode r">curva_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>x, <span class="dt">size =</span> <span class="fl">1.1</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>curva_<span class="dv">1</span></code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-14-1.png" width="480" /></p>
<p>En este caso <span class="math inline">\(\hat{f}\)</span> es una recta, y la podemos usar para hacer predicciones.
Por ejemplo, si tenemos una observación con
<span class="math inline">\(x_0=8\)</span> años de estudio, nuestra predicción del ingreso
<span class="math inline">\(\hat{y}=\hat{f}(8)\)</span> sería</p>
<pre class="sourceCode r"><code class="sourceCode r">lineal &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x,<span class="dt">data =</span> datos)
pred_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">predict</span>(lineal, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x=</span><span class="dv">8</span>))
pred_<span class="dv">1</span></code></pre>
<pre><code>##        1 
## 2193.561</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>curva_<span class="dv">1</span> <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="dv">0</span>, <span class="dt">xend =</span> <span class="dv">8</span>, <span class="dt">y =</span> pred_<span class="dv">1</span>, <span class="dt">yend =</span> pred_<span class="dv">1</span>, <span class="dt">colour =</span> <span class="st">&#39;salmon&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_segment</span>(<span class="dt">x =</span> <span class="dv">8</span>, <span class="dt">xend =</span> <span class="dv">8</span>, <span class="dt">y =</span> <span class="dv">0</span>, <span class="dt">yend =</span> pred_<span class="dv">1</span>, <span class="dt">colour =</span> <span class="st">&#39;salmon&#39;</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">annotate</span>(<span class="st">&#39;text&#39;</span>, <span class="dt">x =</span> <span class="fl">0.5</span>, <span class="dt">y =</span> pred_<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="dv">100</span>, <span class="dt">label =</span> <span class="kw">round</span>(pred_<span class="dv">1</span>, <span class="dv">1</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_point</span>( <span class="dt">x=</span> <span class="dv">8</span>, <span class="dt">y =</span><span class="dv">3200</span>, <span class="dt">col=</span><span class="st">&#39;green&#39;</span>, <span class="dt">size =</span> <span class="dv">4</span>)</code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<p>Si observamos que para esta observación con <span class="math inline">\(x_0=8\)</span>, resulta que
el correspondiente ingreso es <span class="math inline">\(y_0=3200\)</span>, entonces el error sería</p>
<pre class="sourceCode r"><code class="sourceCode r">y_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="dv">3200</span>
y_<span class="dv">0</span> <span class="op">-</span><span class="st"> </span>pred_<span class="dv">1</span></code></pre>
<pre><code>##        1 
## 1006.439</code></pre>
<div class="comentario">
<p>
En aprendizaje buscamos que estos errores sean lo más cercano a cero que sea posible.
</p>
</div>
</div>
<div id="cuantificacion-de-error-o-precision" class="section level2">
<h2><span class="header-section-number">1.4</span> Cuantificación de error o precisión</h2>
<p>El elemento faltante para definir la tarea de aprendizaje supervisado es
qué significa aproximar <em>bien</em> a <span class="math inline">\(f\)</span>, o tener predicciones precisas. Para
esto definimos una <strong>función de pérdida</strong>:</p>
<p><span class="math display">\[L(Y, \hat{f}(X)),\]</span></p>
<p>que nos dice cuánto nos cuesta hacer la predicción <span class="math inline">\(\hat{f}(X)\)</span> cuando el
verdadero valor es <span class="math inline">\(Y\)</span> y las variables de entrada son <span class="math inline">\(X\)</span>.
Una opción conveniente para problemas de regresión
es la pérdida cuadrática:</p>
<p><span class="math display">\[L(Y, \hat{f}(X)) =  (Y - \hat{f}(X))^2\]</span>
Esta es una cantidad aleatoria, de modo que en algunos casos este error
puede ser más grande o más chico. Usualmente buscamos una <span class="math inline">\(\hat{f}\)</span> de
modo que el error promedio sea chico:</p>
<p><span class="math display">\[Err = E[(Y - \hat{f}(X))^2]\]</span></p>
<p><strong>Nota</strong>: Intenta demostrar que bajo error cuadrático medio y suponiendo
el modelo aditivo <span class="math inline">\(Y=f(X)+\epsilon\)</span>, el mejor predictor
de <span class="math inline">\(Y\)</span> es <span class="math inline">\(f(x)= E[Y|X=x]\)</span>. Es decir: lo que nos interesa es aproximar
lo mejor que se pueda la esperanza condicional</p>
</div>
<div id="aprendizaje" class="section level2">
<h2><span class="header-section-number">1.5</span> Tarea de aprendizaje supervisado</h2>
<p>Ahora tenemos los elementos para definir con precisión el problema
de aprendizaje supervisado.</p>
<p>Consideramos un proceso generador de datos <span class="math inline">\((X,Y)\)</span>. En primer lugar,
tenemos datos de los que vamos a aprender.</p>
<p>Supongamos que tenemos un conjunto de datos <em>etiquetados</em> (generados según <span class="math inline">\((X,Y)\)</span>)</p>
<p><span class="math display">\[{\mathcal L}=\{ (x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}), \ldots, (x^{(N)}, y^{(N)}) \}\]</span>
que llamamos <strong>conjunto de entrenamiento</strong>. Nótese que usamos minúsculas
para denotar observaciones particulares de <span class="math inline">\((X,Y)\)</span>.</p>
<p>Un <strong>algoritmo de aprendizaje</strong> es una regla que asigna a cada conjunto de
entrenamiento <span class="math inline">\({\mathcal L}\)</span> una función <span class="math inline">\(\hat{f}\)</span>:</p>
<p><span class="math display">\[{\mathcal L} \to \hat{f}.\]</span></p>
<p>Para medir el desempeño de un predictor particular <span class="math inline">\(\hat{f}\)</span> queremos
ver cómo se va a comportar haciendo predicciones <strong>que no fueron vistos
en el entrenamiento</strong>. Esto es porque esta es justamente la tarea que queremos resolver:</p>
<ul>
<li>Tenemos un conjunto de datos de entrenamiento, y construimos nuestro predictor <span class="math inline">\(\hat{f}\)</span>.</li>
<li>En el futuro llegan nuevos datos, y queremos aplicar nuestro predictor para
hacer predicciones.</li>
</ul>
<p>Entonces, el desempeño del predictor particular <span class="math inline">\(\hat{f}\)</span> se mide como sigue: si
en el futuro observamos otra muestra <span class="math inline">\({\mathcal T}\)</span> (que podemos llamar <strong>muestra de prueba</strong>)</p>
<p><span class="math display">\[{\mathcal T}=\{ (x_0^{(1)},y_0^{(1)}),(x_0^{(2)},y_0^{(2)}), \ldots, (x_0^{(m)}, y_0^{(m)}) \}\]</span></p>
<p>entonces decimos que el <strong>error de predicción</strong> (cuadrático) de <span class="math inline">\(\hat{f}\)</span> para el
ejemplo <span class="math inline">\((x_0^{(j)},y_0^{(j)})\)</span> está dado por
<span class="math display">\[(y_0^{(j)} - \hat{f}(x_0^{(j)}))^2\]</span></p>
<p>y el error sobre la muestra <span class="math inline">\({\mathcal T}\)</span> es</p>
<p><span class="math display">\[\widehat{Err} =   \frac{1}{m}\sum_{j=1}^m (y_0^{(j)} - \hat{f}(x_0^{(j)}))^2\]</span></p>
<p>Es muy importante considerar dos muestras separadas en esta definición:</p>
<ul>
<li><p>No tiene mucho sentido medir el desempeño de nuestro algoritmo sobre
la muestra de entrenamiento, pues el algoritmo puede ver las etiquetas.</p></li>
<li><p>Considerar el error sobre una muestra diferente a la de entrenamiento
nos permite evaluar si nuestro algoritmo <strong>generaliza</strong>, que se puede
pensar como “verdadero” aprendizaje.</p></li>
<li><p>Nótese que <span class="math inline">\(\widehat{Err}\)</span> es una estimación de <span class="math inline">\(Err\)</span> (por la ley de los grandes
números, si <span class="math inline">\({\mathcal T}\)</span> es muestra i.i.d. de <span class="math inline">\((X,Y)\)</span>).</p></li>
</ul>
<p>También consideramos el <strong>error de entrenamiento</strong>, dado por</p>
<p><span class="math display">\[\overline{err} = \frac{1}{N}\sum_{i=1}^N (y^{(i)} - \hat{f}(x^{(i)}))^2\]</span></p>
<p>Nótese sin embargo que el error de entrenamiento <strong>no necesariamente es
una buena estimación del error de prueba, que mide el desempeño verdadero
del modelo</strong>. La razón es que modelos suficientemente flexibles pueden
<strong>memorizar</strong> los datos de entrenamiento en lugar de <strong>aprender</strong> a generalizar
de ellos. Un error de entrenamiento bajo no garantiza un desempeño bueno
en el futuro. Discutiremos este punto más adelante.</p>
<div id="ejemplo-1" class="section level4">
<h4><span class="header-section-number">1.5.0.1</span> Ejemplo</h4>
<p>En el ejemplo que hemos estado usando, ¿que curva preferirías para
predecir, la gris, la roja o la azul? ¿Cuál tiene menor error de entrenamiento?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">280572</span>)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dv">0</span>, <span class="dv">500</span>)
y &lt;-<span class="st"> </span><span class="kw">f</span>(x) <span class="op">+</span><span class="st"> </span>error
datos_entrena &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)
<span class="kw">head</span>(datos_entrena)</code></pre>
<pre><code>##    x          y
## 1  1   86.22033
## 2  7 2353.75863
## 3 10 3078.71029
## 4  0 -397.80229
## 5  0  424.73363
## 6  5 3075.92998</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">curva<span class="fl">.1</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos_entrena,
  <span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;gray&quot;</span>, <span class="dt">span=</span><span class="dv">1</span>, <span class="dt">size=</span><span class="fl">1.1</span>)
curva<span class="fl">.2</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos_entrena,
  <span class="dt">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;red&quot;</span>, <span class="dt">span=</span><span class="fl">0.3</span>, <span class="dt">size=</span><span class="fl">1.1</span>)
curva<span class="fl">.3</span> &lt;-<span class="st"> </span><span class="kw">geom_smooth</span>(<span class="dt">data=</span>datos_entrena,
  <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">color=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">size=</span><span class="fl">1.1</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(datos_entrena, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) <span class="op">+</span><span class="st"> </span><span class="kw">geom_point</span>() <span class="op">+</span><span class="st"> </span>curva<span class="fl">.1</span> <span class="op">+</span><span class="st"> </span>curva<span class="fl">.2</span> <span class="op">+</span><span class="st"> </span>curva<span class="fl">.3</span></code></pre>
<p><img src="01-intro_files/figure-html/unnamed-chunk-20-1.png" width="480" /></p>
<p>Calculamos los errores de entrenamiento de cada curva:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_rojo &lt;-<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> datos_entrena, <span class="dt">span=</span><span class="fl">0.3</span>)
mod_gris &lt;-<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> datos_entrena, <span class="dt">span=</span><span class="dv">1</span>)
mod_recta &lt;-<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">data =</span> datos_entrena)
df_mods &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">nombre =</span> <span class="kw">c</span>(<span class="st">&#39;recta&#39;</span>, <span class="st">&#39;rojo&#39;</span>,<span class="st">&#39;gris&#39;</span>))
df_mods<span class="op">$</span>modelo &lt;-<span class="st"> </span><span class="kw">list</span>(mod_recta, mod_rojo, mod_gris)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">error_f &lt;-<span class="st"> </span><span class="cf">function</span>(df){
  <span class="cf">function</span>(mod){
    preds &lt;-<span class="st"> </span><span class="kw">predict</span>(mod, <span class="dt">newdata =</span> df)
    <span class="kw">round</span>(<span class="kw">sqrt</span>(<span class="kw">mean</span>((preds<span class="op">-</span>df<span class="op">$</span>y)<span class="op">^</span><span class="dv">2</span>)))
  }
}
error_ent &lt;-<span class="st"> </span><span class="kw">error_f</span>(datos_entrena)

df_mods &lt;-<span class="st"> </span>df_mods <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error_entrena =</span> <span class="kw">map_dbl</span>(modelo, error_ent))
df_mods</code></pre>
<pre><code>## # A tibble: 3 x 3
##   nombre modelo      error_entrena
##   &lt;chr&gt;  &lt;list&gt;              &lt;dbl&gt;
## 1 recta  &lt;S3: lm&gt;              782
## 2 rojo   &lt;S3: loess&gt;           189
## 3 gris   &lt;S3: loess&gt;           389</code></pre>
<p>El error de entrenamiento es considerablemente menor para la curva
roja, y es más grande para la recta.</p>
<p>Sin embargo, consideremos que tenemos una nueva muestra (de prueba).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">218052272</span>)
x_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">0</span><span class="op">:</span><span class="dv">13</span>, <span class="dv">100</span>, <span class="dt">replace =</span> T)
error &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x_<span class="dv">0</span>), <span class="dv">0</span>, <span class="dv">500</span>)
y_<span class="dv">0</span> &lt;-<span class="st"> </span><span class="kw">f</span>(x_<span class="dv">0</span>) <span class="op">+</span><span class="st"> </span>error
datos_prueba &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">x =</span> x_<span class="dv">0</span>, <span class="dt">y =</span> y_<span class="dv">0</span>)
datos_prueba</code></pre>
<pre><code>## # A tibble: 100 x 2
##        x      y
##    &lt;int&gt;  &lt;dbl&gt;
##  1     9  2156.
##  2    11  3227.
##  3     3  2382.
##  4    10  3482.
##  5     7  2733.
##  6     7  2326.
##  7    12  3464.
##  8     0  -564.
##  9    10  3296.
## 10     0   366.
## # ... with 90 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">error_p &lt;-<span class="st"> </span><span class="kw">error_f</span>(datos_prueba)
df_mods &lt;-<span class="st"> </span>df_mods <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error_prueba =</span> <span class="kw">map_dbl</span>(modelo, error_p))
df_mods</code></pre>
<pre><code>## # A tibble: 3 x 4
##   nombre modelo      error_entrena error_prueba
##   &lt;chr&gt;  &lt;list&gt;              &lt;dbl&gt;        &lt;dbl&gt;
## 1 recta  &lt;S3: lm&gt;              782          801
## 2 rojo   &lt;S3: loess&gt;           189          628
## 3 gris   &lt;S3: loess&gt;           389          520</code></pre>
</div>
<div id="observaciones" class="section level3 unnumbered">
<h3>Observaciones</h3>

<div class="comentario">
<ul>
<li>En la tarea de aprendizaje estadístico, las dos partes, <strong>entrenamiento</strong>
y <strong>prueba</strong>, son cruciales. Una no tiene sentido sin la otra.</li>
<li>Si no hacemos entrenamiento, no podemos construir
modelos que respondan a los datos. Si no hacemos prueba, no podemos saber si nuestro modelo ajustado en entrenamiento es bueno o malo.
</div></li>
<li><p>El mejor modelo entrenamiento es uno que “sobreajusta” a los datos, pero es
el peor con una muestra de prueba. La curva roja aprende del una componente
de ruido del modelo - lo cual realmente no es aprendizaje.</p></li>
<li><p>El modelo de la recta no es bueno en entrenamiento ni en prueba. Este modelo
no tiene la capacidad para aprender de la señal en los datos.</p></li>
<li><p>El mejor modelo en la muestra de prueba es uno que está entre la recta y
la curva roja en términos de flexibilidad.</p></li>
<li><p>Nuestra intuición para escoger el modelo gris desde el principio se refleja
en que <em>generaliza</em> mejor que los otros, y eso a su vez se refleja en
un error de prueba más bajo.</p></li>
</ul>
</div>
</div>
<div id="por-que-tenemos-errores" class="section level2">
<h2><span class="header-section-number">1.6</span> ¿Por qué tenemos errores?</h2>
<p>¿De dónde
provienen los errores en la predicción? Si establemos que el error es una función
creciente de <span class="math inline">\(Y-\hat{Y}\)</span>, vemos que
<span class="math display">\[ Y-\hat{Y} = f(X) + \epsilon - \hat{f}(X)= (f(X) - \hat{f}(X)) + \epsilon,\]</span>
donde vemos que hay dos componentes que pueden hacer grande a <span class="math inline">\(Y-\hat{Y}\)</span>:</p>
<ul>
<li>La diferencia <span class="math inline">\(f(X) - \hat{f}(X)\)</span> está asociada a <strong>error reducible</strong>, pues
depende de qué tan bien estimemos <span class="math inline">\(f(X)\)</span> con <span class="math inline">\(\hat{f}(X)\)</span></li>
<li>El error aleatorio <span class="math inline">\(\epsilon\)</span>, asociado a <strong>error irreducible</strong>.</li>
</ul>
<p>Cualquiera de estas dos cantidades pueden hacer que nuestras predicciones no sean
precisas.</p>
<p>En nuestro ejemplo anterior, el error reducible:</p>
<ul>
<li>Es grande para el modelo rojo, pues responde demasiado fuerte a ruido en los datos (tiene varianza alta).</li>
<li>Es grande para el modelo de la recta, pues no tiene capacidad para acercarse a
la verdadera curva (está sesgado).</li>
</ul>
<div class="comentario">
<p>
En aprendizaje supervisado, nuestro objetivo es reducir el error reducible tanto como sea posible (obtener la mejor estimación de <span class="math inline"><span class="math inline">\(f\)</span></span>). No podemos hacer nada acerca del error irreducible, pues este se debe a aleatoriedad en el fenómeno o a variables que no conocemos.
</p>
</div>
<div id="notacion" class="section level4 unnumbered">
<h4>Notación</h4>
<p>Las observaciones o datos que usaremos para construir nuestras estimaciones
las denotamos como sigue.</p>
<p>Cada <em>observación</em> (o caso, o ejemplo)
está dada por el valor de una variable de entrada <span class="math inline">\(X\)</span>
y un valor de la variable de salida <span class="math inline">\(Y\)</span>.
Cuando tenemos <span class="math inline">\(N\)</span> ejemplos de entrenamiento, los
escribimos como los pares
<span class="math display">\[(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}) \ldots, (x^{(N)},y^{(N)})\]</span>.
Cuando los datos de entrada contienen <span class="math inline">\(p\)</span> variables o atributos, escribimos
<span class="math display">\[x^{(i)} = (x_1^{(i)}, x_2^{(i)},\ldots, x_p^{(i)})\]</span></p>
<p>Escribimos también la matriz de entradas de dimensión Nxp:
<span class="math display">\[\underline{X} =  \left ( \begin{array}{cccc}
x_1^{(1)} &amp; x_2^{(1)} &amp; \ldots  &amp; x_p^{(1)} \\
x_1^{(2)} &amp; x_2^{(2)} &amp; \ldots  &amp; x_p^{(2)}\\
\vdots &amp; \vdots &amp;   &amp;  \vdots \\
x_1^{(N)} &amp; x_2^{(N)} &amp; \ldots  &amp; x_p^{(N)} \\
 \end{array} \right)\]</span>
y <span class="math display">\[\underline{y} =(y^{(1)},y^{(2)}, \ldots, y^{(N)})^t.\]</span></p>
<p>Adicionalmente, usamos la notación</p>
<p><span class="math display">\[{\mathcal L}=\{ (x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \ldots, (x^{(N)},y^{(N)}) \}\]</span></p>
<p>para denotar al conjunto de datos con los que construimos nuestro
modelo. A este conjunto le llamaremos <em>conjunto o muestra de entrenamiento</em>
(learning set)</p>
</div>
</div>
<div id="resumen" class="section level2">
<h2><span class="header-section-number">1.7</span> Resumen</h2>
<ul>
<li><p>Aprendizaje de máquina: algoritmos que aprenden de los datos para predecir cantidades
numéricas, o clasificar (aprendizaje supervisado), o para encontrar estructura en los
datos (aprendizaje no supervisado).</p></li>
<li>En aprendizaje supervisado, el esquema general es:
<ul>
<li>Un algoritmo aprende de una
muestra de entrenamiento <span class="math inline">\({\mathcal L}\)</span>, que es generada por el proceso generador de datos que nos interesa. Eso quiere decir que produce una función <span class="math inline">\(\hat{f}\)</span> (a partir de <span class="math inline">\({\mathcal L}\)</span>) que nos sirve para hacer predicciones <span class="math inline">\(x \to \hat{f}(x)\)</span> de <span class="math inline">\(y\)</span></li>
<li>El error de predicción del algoritmo es <span class="math inline">\(Err\)</span>, que mide en promedio qué tan lejos están las predicciones de valores reales.</li>
<li>Para estimar esta cantidad usamos una muestra de prueba <span class="math inline">\({\mathcal T}\)</span>, que
es independiente de <span class="math inline">\({\mathcal L}\)</span>.</li>
<li>Esta es porque nos interesa el desempeño futuro de <span class="math inline">\(\hat{f}\)</span> para nuevos casos
que el algoritmo no ha visto (esto es aprender).</li>
</ul></li>
<li><p>El error en la muestra de entrenamiento no necesariamente es buen indicador
del desempeño futuro de nuestro algoritmo.</p></li>
<li><p>Para obtener las mejores predicciones posibles, es necesario que el algoritmo
sea capaz de capturar patrones en los datos, pero no tanto que tienda a absorber ruido
en la estimación - es un balance de complejidad y rigidez. En términos estadísticos,
se trata de un balance de varianza y sesgo.</p></li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regresion-lineal.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-verano-2018/edit/master/01-intro.Rmd",
"text": "Edit"
},
"download": ["am-curso-verano.pdf", "am-curso-verano.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
