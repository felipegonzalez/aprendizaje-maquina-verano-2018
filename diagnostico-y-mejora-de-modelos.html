<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Minicurso de verano de Aprendizaje Máquina</title>
  <meta name="description" content="Minicurso de Aprendizaje Máquina, ITAM 2018.">
  <meta name="generator" content="bookdown 0.7.12 and GitBook 2.6.7">

  <meta property="og:title" content="Minicurso de verano de Aprendizaje Máquina" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  <meta name="github-repo" content="felipegonzalez/aprendizaje-maquina-verano-2018" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Minicurso de verano de Aprendizaje Máquina" />
  
  <meta name="twitter:description" content="Minicurso de Aprendizaje Máquina, ITAM 2018." />
  

<meta name="author" content="Felipe González">


<meta name="date" content="2018-06-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="descenso-estocastico.html">
<link rel="next" href="metodos-basados-en-arboles.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.7.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotlyjs-1.29.2/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotlyjs-1.29.2/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; position: absolute; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; }
pre.numberSource a.sourceLine:empty
  { position: absolute; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: absolute; left: -5em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Minicurso aprendizaje máquina</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Temario</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#referencias"><i class="fa fa-check"></i>Referencias</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software"><i class="fa fa-check"></i>Software</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduccion.html"><a href="introduccion.html"><i class="fa fa-check"></i><b>1</b> Introducción</a><ul>
<li class="chapter" data-level="1.1" data-path="introduccion.html"><a href="introduccion.html#que-es-aprendizaje-de-maquina-machine-learning"><i class="fa fa-check"></i><b>1.1</b> ¿Qué es aprendizaje de máquina (machine learning)?</a></li>
<li class="chapter" data-level="1.2" data-path="introduccion.html"><a href="introduccion.html#aprendizaje-supervisado"><i class="fa fa-check"></i><b>1.2</b> Aprendizaje Supervisado</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduccion.html"><a href="introduccion.html#proceso-generador-de-datos-modelo-teorico"><i class="fa fa-check"></i><b>1.2.1</b> Proceso generador de datos (modelo teórico)</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduccion.html"><a href="introduccion.html#predicciones"><i class="fa fa-check"></i><b>1.3</b> Predicciones</a></li>
<li class="chapter" data-level="1.4" data-path="introduccion.html"><a href="introduccion.html#cuantificacion-de-error-o-precision"><i class="fa fa-check"></i><b>1.4</b> Cuantificación de error o precisión</a></li>
<li class="chapter" data-level="1.5" data-path="introduccion.html"><a href="introduccion.html#aprendizaje"><i class="fa fa-check"></i><b>1.5</b> Tarea de aprendizaje supervisado</a><ul>
<li class="chapter" data-level="" data-path="introduccion.html"><a href="introduccion.html#observaciones"><i class="fa fa-check"></i>Observaciones</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="introduccion.html"><a href="introduccion.html#por-que-tenemos-errores"><i class="fa fa-check"></i><b>1.6</b> ¿Por qué tenemos errores?</a></li>
<li class="chapter" data-level="1.7" data-path="introduccion.html"><a href="introduccion.html#resumen"><i class="fa fa-check"></i><b>1.7</b> Resumen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regresion-lineal.html"><a href="regresion-lineal.html"><i class="fa fa-check"></i><b>2</b> Regresión lineal</a><ul>
<li class="chapter" data-level="2.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#introduccion-1"><i class="fa fa-check"></i><b>2.1</b> Introducción</a></li>
<li class="chapter" data-level="2.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#aprendizaje-de-coeficientes-ajuste"><i class="fa fa-check"></i><b>2.2</b> Aprendizaje de coeficientes (ajuste)</a></li>
<li class="chapter" data-level="2.3" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente"><i class="fa fa-check"></i><b>2.3</b> Descenso en gradiente</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#seleccion-de-tamano-de-paso-eta"><i class="fa fa-check"></i><b>2.3.1</b> Selección de tamaño de paso <span class="math inline">\(\eta\)</span></a></li>
<li class="chapter" data-level="2.3.2" data-path="regresion-lineal.html"><a href="regresion-lineal.html#funciones-de-varias-variables"><i class="fa fa-check"></i><b>2.3.2</b> Funciones de varias variables</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regresion-lineal.html"><a href="regresion-lineal.html#descenso-en-gradiente-para-regresion-lineal"><i class="fa fa-check"></i><b>2.4</b> Descenso en gradiente para regresión lineal</a></li>
<li class="chapter" data-level="2.5" data-path="regresion-lineal.html"><a href="regresion-lineal.html#normalizacion-de-entradas"><i class="fa fa-check"></i><b>2.5</b> Normalización de entradas</a></li>
<li class="chapter" data-level="2.6" data-path="regresion-lineal.html"><a href="regresion-lineal.html#interpretacion-de-modelos-lineales"><i class="fa fa-check"></i><b>2.6</b> Interpretación de modelos lineales</a></li>
<li class="chapter" data-level="2.7" data-path="regresion-lineal.html"><a href="regresion-lineal.html#por-que-el-modelo-lineal-funciona-bien-muchas-veces"><i class="fa fa-check"></i><b>2.7</b> ¿Por qué el modelo lineal funciona bien (muchas veces)?</a><ul>
<li class="chapter" data-level="2.7.1" data-path="regresion-lineal.html"><a href="regresion-lineal.html#k-vecinos-mas-cercanos"><i class="fa fa-check"></i><b>2.7.1</b> k vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="regresion-lineal.html"><a href="regresion-lineal.html#ejercicio-1"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="regresion-logistica.html"><a href="regresion-logistica.html"><i class="fa fa-check"></i><b>3</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#el-problema-de-clasificacion"><i class="fa fa-check"></i><b>3.1</b> El problema de clasificación</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#que-estimar-en-problemas-de-clasificacion"><i class="fa fa-check"></i>¿Qué estimar en problemas de clasificación?</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#estimacion-de-probabilidades-de-clase"><i class="fa fa-check"></i><b>3.2</b> Estimación de probabilidades de clase</a><ul>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejemplo-9"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="3.2.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#k-vecinos-mas-cercanos-1"><i class="fa fa-check"></i><b>3.2.1</b> k-vecinos más cercanos</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-para-modelos-de-clasificacion"><i class="fa fa-check"></i><b>3.3</b> Error para modelos de clasificación</a><ul>
<li class="chapter" data-level="3.3.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-2"><i class="fa fa-check"></i><b>3.3.1</b> Ejercicio</a></li>
<li class="chapter" data-level="3.3.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#error-de-clasificacion-y-funcion-de-perdida-0-1"><i class="fa fa-check"></i><b>3.3.2</b> Error de clasificación y función de pérdida 0-1</a></li>
<li class="chapter" data-level="3.3.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#discusion-relacion-entre-devianza-y-error-de-clasificacion"><i class="fa fa-check"></i><b>3.3.3</b> Discusión: relación entre devianza y error de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-1"><i class="fa fa-check"></i><b>3.4</b> Regresión logística</a><ul>
<li class="chapter" data-level="3.4.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-simple"><i class="fa fa-check"></i><b>3.4.1</b> Regresión logística simple</a></li>
<li class="chapter" data-level="3.4.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#funcion-logistica"><i class="fa fa-check"></i><b>3.4.2</b> Función logística</a></li>
<li class="chapter" data-level="3.4.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-2"><i class="fa fa-check"></i><b>3.4.3</b> Regresión logística</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="regresion-logistica.html"><a href="regresion-logistica.html#aprendizaje-de-coeficientes-para-regresion-logistica-binomial."><i class="fa fa-check"></i><b>3.5</b> Aprendizaje de coeficientes para regresión logística (binomial).</a></li>
<li class="chapter" data-level="3.6" data-path="regresion-logistica.html"><a href="regresion-logistica.html#observaciones-adicionales"><i class="fa fa-check"></i><b>3.6</b> Observaciones adicionales</a></li>
<li class="chapter" data-level="" data-path="regresion-logistica.html"><a href="regresion-logistica.html#ejercicio-datos-de-diabetes"><i class="fa fa-check"></i>Ejercicio: datos de diabetes</a></li>
<li class="chapter" data-level="3.7" data-path="regresion-logistica.html"><a href="regresion-logistica.html#mas-sobre-problemas-de-clasificacion"><i class="fa fa-check"></i><b>3.7</b> Más sobre problemas de clasificación</a><ul>
<li class="chapter" data-level="3.7.1" data-path="regresion-logistica.html"><a href="regresion-logistica.html#analisis-de-error-para-clasificadores-binarios"><i class="fa fa-check"></i><b>3.7.1</b> Análisis de error para clasificadores binarios</a></li>
<li class="chapter" data-level="3.7.2" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-para-problemas-de-mas-de-2-clases"><i class="fa fa-check"></i><b>3.7.2</b> Regresión logística para problemas de más de 2 clases</a></li>
<li class="chapter" data-level="3.7.3" data-path="regresion-logistica.html"><a href="regresion-logistica.html#regresion-logistica-multinomial"><i class="fa fa-check"></i><b>3.7.3</b> Regresión logística multinomial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html"><i class="fa fa-check"></i><b>4</b> Regresión regularizada</a><ul>
<li class="chapter" data-level="4.0.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#sesgo-y-varianza-en-modelos-lineales"><i class="fa fa-check"></i><b>4.0.1</b> Sesgo y varianza en modelos lineales</a></li>
<li class="chapter" data-level="4.0.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#reduciendo-varianza-de-los-coeficientes"><i class="fa fa-check"></i><b>4.0.2</b> Reduciendo varianza de los coeficientes</a></li>
<li class="chapter" data-level="4.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-ridge"><i class="fa fa-check"></i><b>4.1</b> Regularización ridge</a><ul>
<li class="chapter" data-level="4.1.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#seleccion-de-coeficiente-de-regularizacion"><i class="fa fa-check"></i><b>4.1.1</b> Selección de coeficiente de regularización</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#entrenamiento-validacion-y-prueba"><i class="fa fa-check"></i><b>4.2</b> Entrenamiento, Validación y Prueba</a><ul>
<li class="chapter" data-level="4.2.1" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#validacion-cruzada"><i class="fa fa-check"></i><b>4.2.1</b> Validación cruzada</a></li>
<li class="chapter" data-level="" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#ejercicio-4"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="regresion-regularizada.html"><a href="regresion-regularizada.html#regularizacion-lasso"><i class="fa fa-check"></i><b>4.3</b> Regularización lasso</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html"><i class="fa fa-check"></i><b>5</b> Descenso estocástico</a><ul>
<li class="chapter" data-level="5.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#algoritmo-de-descenso-estocastico"><i class="fa fa-check"></i><b>5.1</b> Algoritmo de descenso estocástico</a></li>
<li class="chapter" data-level="5.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#por-que-usar-descenso-estocastico-por-minilotes"><i class="fa fa-check"></i><b>5.2</b> ¿Por qué usar descenso estocástico por minilotes?</a></li>
<li class="chapter" data-level="5.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#escogiendo-la-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.3</b> Escogiendo la tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#mejoras-al-algoritmo-de-descenso-estocastico."><i class="fa fa-check"></i><b>5.4</b> Mejoras al algoritmo de descenso estocástico.</a><ul>
<li class="chapter" data-level="5.4.1" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#decaimiento-de-tasa-de-aprendizaje"><i class="fa fa-check"></i><b>5.4.1</b> Decaimiento de tasa de aprendizaje</a></li>
<li class="chapter" data-level="5.4.2" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#momento"><i class="fa fa-check"></i><b>5.4.2</b> Momento</a></li>
<li class="chapter" data-level="5.4.3" data-path="descenso-estocastico.html"><a href="descenso-estocastico.html#otras-variaciones"><i class="fa fa-check"></i><b>5.4.3</b> Otras variaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html"><i class="fa fa-check"></i><b>6</b> Diagnóstico y mejora de modelos</a><ul>
<li class="chapter" data-level="6.1" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#aspectos-generales"><i class="fa fa-check"></i><b>6.1</b> Aspectos generales</a></li>
<li class="chapter" data-level="6.2" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#que-hacer-cuando-el-desempeno-no-es-satisfactorio"><i class="fa fa-check"></i><b>6.2</b> ¿Qué hacer cuando el desempeño no es satisfactorio?</a></li>
<li class="chapter" data-level="6.3" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#pipeline-de-procesamiento"><i class="fa fa-check"></i><b>6.3</b> Pipeline de procesamiento</a></li>
<li class="chapter" data-level="6.4" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#diagnosticos-sesgo-y-varianza"><i class="fa fa-check"></i><b>6.4</b> Diagnósticos: sesgo y varianza</a></li>
<li class="chapter" data-level="6.5" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#refinando-el-pipeline"><i class="fa fa-check"></i><b>6.5</b> Refinando el pipeline</a></li>
<li class="chapter" data-level="6.6" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#consiguiendo-mas-datos"><i class="fa fa-check"></i><b>6.6</b> Consiguiendo más datos</a></li>
<li class="chapter" data-level="6.7" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#usar-datos-adicionales"><i class="fa fa-check"></i><b>6.7</b> Usar datos adicionales</a></li>
<li class="chapter" data-level="6.8" data-path="diagnostico-y-mejora-de-modelos.html"><a href="diagnostico-y-mejora-de-modelos.html#examen-de-modelo-y-analisis-de-errores"><i class="fa fa-check"></i><b>6.8</b> Examen de modelo y Análisis de errores</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html"><i class="fa fa-check"></i><b>7</b> Métodos basados en árboles</a><ul>
<li class="chapter" data-level="7.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion-y-clasificacion."><i class="fa fa-check"></i><b>7.1</b> Árboles para regresión y clasificación.</a><ul>
<li class="chapter" data-level="7.1.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-clasificacion"><i class="fa fa-check"></i><b>7.1.1</b> Árboles para clasificación</a></li>
<li class="chapter" data-level="7.1.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tipos-de-particion"><i class="fa fa-check"></i><b>7.1.2</b> Tipos de partición</a></li>
<li class="chapter" data-level="7.1.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#medidas-de-impureza"><i class="fa fa-check"></i><b>7.1.3</b> Medidas de impureza</a></li>
<li class="chapter" data-level="7.1.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#reglas-de-particion-y-tamano-del-arobl"><i class="fa fa-check"></i><b>7.1.4</b> Reglas de partición y tamaño del árobl</a></li>
<li class="chapter" data-level="7.1.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#costo---complejidad-breiman"><i class="fa fa-check"></i><b>7.1.5</b> Costo - Complejidad (Breiman)</a></li>
<li class="chapter" data-level="7.1.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#opcional-predicciones-con-cart"><i class="fa fa-check"></i><b>7.1.6</b> (Opcional) Predicciones con CART</a></li>
<li class="chapter" data-level="7.1.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#arboles-para-regresion"><i class="fa fa-check"></i><b>7.1.7</b> Árboles para regresión</a></li>
<li class="chapter" data-level="7.1.8" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#variabilidad-en-el-proceso-de-construccion"><i class="fa fa-check"></i><b>7.1.8</b> Variabilidad en el proceso de construcción</a></li>
<li class="chapter" data-level="7.1.9" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#relaciones-lineales"><i class="fa fa-check"></i><b>7.1.9</b> Relaciones lineales</a></li>
<li class="chapter" data-level="7.1.10" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles"><i class="fa fa-check"></i><b>7.1.10</b> Ventajas y desventajas de árboles</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bagging-de-arboles"><i class="fa fa-check"></i><b>7.2</b> Bagging de árboles</a><ul>
<li class="chapter" data-level="7.2.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-28"><i class="fa fa-check"></i><b>7.2.1</b> Ejemplo</a></li>
<li class="chapter" data-level="7.2.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mejorando-bagging"><i class="fa fa-check"></i><b>7.2.2</b> Mejorando bagging</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#bosques-aleatorios"><i class="fa fa-check"></i><b>7.3</b> Bosques aleatorios</a><ul>
<li class="chapter" data-level="7.3.1" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#sabiduria-de-las-masas"><i class="fa fa-check"></i><b>7.3.1</b> Sabiduría de las masas</a></li>
<li class="chapter" data-level="7.3.2" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ejemplo-29"><i class="fa fa-check"></i><b>7.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="7.3.3" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#mas-detalles-de-bosques-aleatorios."><i class="fa fa-check"></i><b>7.3.3</b> Más detalles de bosques aleatorios.</a></li>
<li class="chapter" data-level="7.3.4" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#importancia-de-variables"><i class="fa fa-check"></i><b>7.3.4</b> Importancia de variables</a></li>
<li class="chapter" data-level="7.3.5" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ajustando-arboles-aleatorios."><i class="fa fa-check"></i><b>7.3.5</b> Ajustando árboles aleatorios.</a></li>
<li class="chapter" data-level="7.3.6" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#ventajas-y-desventajas-de-arboles-aleatorios"><i class="fa fa-check"></i><b>7.3.6</b> Ventajas y desventajas de árboles aleatorios</a></li>
<li class="chapter" data-level="7.3.7" data-path="metodos-basados-en-arboles.html"><a href="metodos-basados-en-arboles.html#tarea-para-23-de-octubre"><i class="fa fa-check"></i><b>7.3.7</b> Tarea (para 23 de octubre)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html"><i class="fa fa-check"></i><b>8</b> Validación de modelos: problemas comunes</a><ul>
<li class="chapter" data-level="8.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-de-datos"><i class="fa fa-check"></i><b>8.1</b> Filtración de datos</a></li>
<li class="chapter" data-level="8.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#series-de-tiempo"><i class="fa fa-check"></i><b>8.2</b> Series de tiempo</a></li>
<li class="chapter" data-level="8.3" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#filtracion-en-el-preprocesamiento"><i class="fa fa-check"></i><b>8.3</b> Filtración en el preprocesamiento</a></li>
<li class="chapter" data-level="8.4" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#uso-de-variables-fuera-de-rango-temporal"><i class="fa fa-check"></i><b>8.4</b> Uso de variables fuera de rango temporal</a></li>
<li class="chapter" data-level="8.5" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#datos-en-conglomerados-y-muestreo-complejo"><i class="fa fa-check"></i><b>8.5</b> Datos en conglomerados y muestreo complejo</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-32"><i class="fa fa-check"></i>Ejemplo</a></li>
<li class="chapter" data-level="8.5.1" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#censura-y-evaluacion-incompleta"><i class="fa fa-check"></i><b>8.5.1</b> Censura y evaluación incompleta</a></li>
<li class="chapter" data-level="8.5.2" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejemplo-tiendas-cerradas"><i class="fa fa-check"></i><b>8.5.2</b> Ejemplo: tiendas cerradas</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#muestras-de-validacion-chicas"><i class="fa fa-check"></i><b>8.6</b> Muestras de validación chicas</a><ul>
<li class="chapter" data-level="" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#ejercicio-5"><i class="fa fa-check"></i>Ejercicio</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#otros-ejemplos"><i class="fa fa-check"></i><b>8.7</b> Otros ejemplos</a></li>
<li class="chapter" data-level="8.8" data-path="validacion-de-modelos-problemas-comunes.html"><a href="validacion-de-modelos-problemas-comunes.html#resumen-1"><i class="fa fa-check"></i><b>8.8</b> Resumen</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Publicado con bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Minicurso de verano de Aprendizaje Máquina</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="diagnostico-y-mejora-de-modelos" class="section level1">
<h1><span class="header-section-number">Sección 6</span> Diagnóstico y mejora de modelos</h1>
<div id="aspectos-generales" class="section level2">
<h2><span class="header-section-number">6.1</span> Aspectos generales</h2>
<p>Al comenzar un proyecto de machine learning, las primeras consideraciones deben ser:</p>

<div class="comentario">
<ol style="list-style-type: decimal">
<li><p>Establecer métricas de error apropiadas para el problema, y cuál es el máximo
valor de este error requerido para nuestra aplicación.</p></li>
<li>Construir un <em>pipeline</em> lo antes posible que vaya de datos hasta medición
de calidad de los modelos. Este pipeline deberá, al menos, incluir cálculos de entradas,
medición de desempeño de los modelos y cálculos de otros diagnósticos (como error
de entrenamiento, convergencia de algoritmos, etc.)
</div></li>
</ol>
<p>En general, es difícil preveer exactamente qué va a funcionar para
un problema particular, y los diagnósticos que veremos
requieren de haber ajustado modelos. Nuestra primera
recomendación para ir hacia un modelo de mejor desempeño es:</p>
<p>Es mejor y más rápido comenzar rápido, aún con un modelo simple, con entradas {} (no muy refinadas), y con los datos que tenemos a mano. De esta forma podemos aprender más rápido. Demasiado tiempo pensando, discutiendo, o diseñando qué algoritmo
deberíamos usar, cómo deberíamos construir las entradas, etc. es muchas veces
tiempo perdido.</p>
<p>Con el pipeline establecido, si el resultado no es satisfactorio, entonces
tenemos que tomar decisiones para mejorar.</p>
</div>
<div id="que-hacer-cuando-el-desempeno-no-es-satisfactorio" class="section level2">
<h2><span class="header-section-number">6.2</span> ¿Qué hacer cuando el desempeño no es satisfactorio?</h2>
<p>Supongamos que tenemos un clasificador construido con regresión
logística regularizada, y que cuando lo aplicamos a nuestra muestra
de prueba el desempeño
es malo. ¿Qué hacer?</p>
<p>Algunas opciones:</p>
<ul>
<li>Conseguir más datos de entrenamiento.</li>
<li>Reducir el número de entradas por algún método (eliminación manual, componentes principales, etc.)</li>
<li>Construir más entradas utilizando distintos enfoques o fuentes de datos.</li>
<li>Incluir variables derivadas adicionales e interacciones.</li>
<li>Intentar construir una red neuronal para predecir (otro método).</li>
<li>Aumentar la regularización.</li>
<li>Disminuir la regularización.</li>
<li>Correr más tiempo el algoritmo de ajuste.</li>
</ul>
<p>¿Con cuál empezar? Cada una de estas estrategias intenta arreglar
distintos problemas. En lugar de intentar al azar distintas cosas, que
consumen tiempo y dinero y no necesariamente nos van a llevar a mejoras,
a continuación veremos diagnósticos y recetas
que nos sugieren la mejor manera de usar nuestro tiempo para mejorar
nuestros modelos.</p>
<p>Usaremos el siguiente ejemplo para ilustrar los conceptos:</p>
<div id="ejemplo-22" class="section level4 unnumbered">
<h4>Ejemplo</h4>
<p>Nos interesa hacer una predicción de polaridad de críticas o comentarios
de pelíıculas: buscamos clasificar una reseña como positiva o negativa dependiendo de su contenido. Tenemos dos grupos de reseñas separadas en positivas y negativas
(estos datos fueron etiquetados por una persona).</p>
<p>Cada reseña está un archivo de texto, y tenemos 1000 de cada tipo:</p>
<pre class="sourceCode r"><code class="sourceCode r">negativos &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="st">&#39;./datos/sentiment/neg&#39;</span>, <span class="dt">full.names =</span> <span class="ot">TRUE</span>)
positivos &lt;-<span class="st"> </span><span class="kw">list.files</span>(<span class="st">&#39;./datos/sentiment/pos&#39;</span>, <span class="dt">full.names =</span> <span class="ot">TRUE</span>)
<span class="kw">head</span>(negativos)</code></pre>
<pre><code>## [1] &quot;./datos/sentiment/neg/cv000_29416.txt&quot;
## [2] &quot;./datos/sentiment/neg/cv001_19502.txt&quot;
## [3] &quot;./datos/sentiment/neg/cv002_17424.txt&quot;
## [4] &quot;./datos/sentiment/neg/cv003_12683.txt&quot;
## [5] &quot;./datos/sentiment/neg/cv004_12641.txt&quot;
## [6] &quot;./datos/sentiment/neg/cv005_29357.txt&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(positivos)</code></pre>
<pre><code>## [1] &quot;./datos/sentiment/pos/cv000_29590.txt&quot;
## [2] &quot;./datos/sentiment/pos/cv001_18431.txt&quot;
## [3] &quot;./datos/sentiment/pos/cv002_15918.txt&quot;
## [4] &quot;./datos/sentiment/pos/cv003_11664.txt&quot;
## [5] &quot;./datos/sentiment/pos/cv004_11636.txt&quot;
## [6] &quot;./datos/sentiment/pos/cv005_29443.txt&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(negativos)</code></pre>
<pre><code>## [1] 1000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">length</span>(positivos)</code></pre>
<pre><code>## [1] 1000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">read_file</span>(negativos[<span class="dv">1</span>])</code></pre>
<p>[1] “plot : two teen couples go to a church party , drink and then drive . get into an accident . of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . ’s the deal ? the movie and &quot; sorta &quot; find out . . . : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . is what makes this review an even harder one to write , since i generally applaud films which attempt to break the mold , mess with your head and such ( lost highway &amp; memento ) , but there are good and bad ways of making all types of films , and these folks just didn’t snag this one correctly . seem to have taken this pretty neat concept , but executed it terribly . what are the problems with the movie ? , its main problem is that it’s simply too jumbled . starts off &quot; normal &quot; but then downshifts into this &quot; fantasy &quot; world in which you , as an audience member , have no idea what’s going on . are dreams , there are characters coming back from the dead , there are others who look like the dead , there are strange apparitions , there are disappearances , there are a looooot of chase scenes , there are tons of weird things that happen , and most of it is simply not explained . i personally don’t mind trying to unravel a film every now and then , but when all it does is give me the same clue over and over again , i get kind of fed up after a while , which is this film’s biggest problem . ’s obviously got this big secret to hide , but it seems to want to hide it completely until its final five minutes . do they make things entertaining , thrilling or even engaging , in the meantime ? really . sad part is that the arrow and i both dig on flicks like this , so we actually figured most of it out by the half-way point , so all of the strangeness after that did start to make a little bit of sense , but it still didn’t the make the film all that more entertaining . guess the bottom line with movies like this is that you should always make sure that the audience is &quot; into it &quot; even before they are given the secret password to enter your world of understanding . mean , showing melissa sagemiller running away from visions for about 20 minutes throughout the movie is just plain lazy ! ! , we get it . . . there people chasing her and we don’t know who they are . we really need to see it over and over again ? about giving us different scenes offering further insight into all of the strangeness going down in the movie ? , the studio took this film away from its director and chopped it up themselves , and it shows . might’ve been a pretty decent teen mind-fuck movie in here somewhere , but i guess &quot; the suits &quot; decided that turning it into a music video with little edge , would make more sense . actors are pretty good for the most part , although wes bentley just seemed to be playing the exact same character that he did in american beauty , only in a new neighborhood . my biggest kudos go out to sagemiller , who holds her own throughout the entire film , and actually has you feeling her character’s unraveling . , the film doesn’t stick because it doesn’t entertain , it’s confusing , it rarely excites and it feels pretty redundant for most of its runtime , despite a pretty cool ending and explanation to all of the craziness that came before it . , and by the way , this is not a horror or teen slasher flick . . . it’s packaged to look that way because someone is apparently assuming that the genre is still hot with the kids . also wrapped production two years ago and has been sitting on the shelves ever since . . . . skip ! ’s joblo coming from ? nightmare of elm street 3 ( 7/10 ) - blair witch 2 ( 7/10 ) - the crow ( 9/10 ) - the crow : salvation ( 4/10 ) - lost highway ( 10/10 ) - memento ( 10/10 ) - the others ( 9/10 ) - stir of echoes ( 8/10 ) ”</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">read_file</span>(positivos[<span class="dv">1</span>])</code></pre>
<p>[1] “films adapted from comic books have had plenty of success , whether they’re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there’s never really been a comic book like from hell before . starters , it was created by alan moore ( and eddie campbell ) , who brought the medium to a whole new level in the mid ‘80s with a 12-part series called the watchmen . say moore and campbell thoroughly researched the subject of jack the ripper would be like saying michael jackson is starting to look a little odd . book ( or &quot; graphic novel , &quot; if you will ) is over 500 pages long and includes nearly 30 more that consist of nothing but footnotes . other words , don’t dismiss this film because of its source . you can get past the whole comic book thing , you might find another stumbling block in from hell’s directors , albert and allen hughes . the hughes brothers to direct this seems almost as ludicrous as casting carrot top in , well , anything , but riddle me this : who better to direct a film that’s set in the ghetto and features really violent street crime than the mad geniuses behind menace ii society ? ghetto in question is , of course , whitechapel in 1888 london’s east end . ’s a filthy , sooty place where the whores ( called &quot; unfortunates &quot; ) are starting to get a little nervous about this mysterious psychopath who has been carving through their profession with surgical precision . the first stiff turns up , copper peter godley ( robbie coltrane , the world is not enough ) calls in inspector frederick abberline ( johnny depp , blow ) to crack the case . , a widower , has prophetic dreams he unsuccessfully tries to quell with copious amounts of absinthe and opium . arriving in whitechapel , he befriends an unfortunate named mary kelly ( heather graham , say it isn’t so ) and proceeds to investigate the horribly gruesome crimes that even the police surgeon can’t stomach . don’t think anyone needs to be briefed on jack the ripper , so i won’t go into the particulars here , other than to say moore and campbell have a unique and interesting theory about both the identity of the killer and the reasons he chooses to slay . the comic , they don’t bother cloaking the identity of the ripper , but screenwriters terry hayes ( vertical limit ) and rafael yglesias ( les mis ? rables ) do a good job of keeping him hidden from viewers until the very end . ’s funny to watch the locals blindly point the finger of blame at jews and indians because , after all , an englishman could never be capable of committing such ghastly acts . from hell’s ending had me whistling the stonecutters song from the simpsons for days ( &quot; who holds back the electric car/who made steve guttenberg a star ? &quot; ) . ’t worry - it’ll all make sense when you see it . onto from hell’s appearance : it’s certainly dark and bleak enough , and it’s surprising to see how much more it looks like a tim burton film than planet of the apes did ( at times , it seems like sleepy hollow 2 ) . print i saw wasn’t completely finished ( both color and music had not been finalized , so no comments about marilyn manson ) , but cinematographer peter deming ( don’t say a word ) ably captures the dreariness of victorian-era london and helped make the flashy killing scenes remind me of the crazy flashbacks in twin peaks , even though the violence in the film pales in comparison to that in the black-and-white comic . winner martin childs’ ( shakespeare in love ) production design turns the original prague surroundings into one creepy place . the acting in from hell is solid , with the dreamy depp turning in a typically strong performance and deftly handling a british accent . holm ( joe gould’s secret ) and richardson ( 102 dalmatians ) log in great supporting roles , but the big surprise here is graham . cringed the first time she opened her mouth , imagining her attempt at an irish accent , but it actually wasn’t half bad . film , however , is all good . 2 : 00 - r for strong violence/gore , sexuality , language and drug content ”</p>
<ul>
<li><p>Consideremos primero la métrica de error, que depende de nuestra aplicación. En
este caso, quisiéramos hacer dar una calificación a cada película basada en el
% de reseñas positivas que tiene. Supongamos que se ha decidido que
necesitamos al menos una tasa de correctos de 90% para que el score sea confiable
(cómo calcularías algo así?).</p></li>
<li><p>Ahora necesitamos construir un pipeline para obtener las primeras predicciones.
Tenemos que pensar qué entradas podríamos construir.</p></li>
</ul>
</div>
</div>
<div id="pipeline-de-procesamiento" class="section level2">
<h2><span class="header-section-number">6.3</span> Pipeline de procesamiento</h2>
<p>Empezamos por construir funciones para leer datos (ver script).
Construimos un data frame:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&#39;./scripts/funciones_sentiment.R&#39;</span>)
df &lt;-<span class="st"> </span><span class="kw">prep_df</span>(<span class="st">&#39;./datos/sentiment/&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">unnest</span>(texto)
<span class="kw">nrow</span>(df)</code></pre>
<p>[1] 2000</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str_sub</span>(df<span class="op">$</span>texto[<span class="dv">1</span>], <span class="dv">1</span>, <span class="dv">200</span>)</code></pre>
<p>[1] “Review films adapted from comic books have had plenty of success , whether they’re about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost wor”</p>
<p>Ahora separamos una muestra de prueba (y una de entrenamiento más chica
para simular después el proceso de recoger más datos):</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">94512</span>)
df<span class="op">$</span>muestra &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&#39;entrena&#39;</span>, <span class="st">&#39;prueba&#39;</span>), <span class="dv">2000</span>, <span class="dt">prob =</span> <span class="kw">c</span>(<span class="fl">0.8</span>, <span class="fl">0.2</span>),
                     <span class="dt">replace =</span> <span class="ot">TRUE</span>)
<span class="kw">table</span>(df<span class="op">$</span>muestra)</code></pre>
<pre><code>## 
## entrena  prueba 
##    1575     425</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">df_ent &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(muestra <span class="op">==</span><span class="st"> &#39;entrena&#39;</span>)
df_pr &lt;-<span class="st"> </span>df <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(muestra <span class="op">==</span><span class="st"> &#39;prueba&#39;</span>)
df_ent &lt;-<span class="st"> </span><span class="kw">sample_n</span>(df_ent, <span class="kw">nrow</span>(df_ent)) <span class="co">#permutamos al azar</span>
df_ent_grande &lt;-<span class="st"> </span>df_ent
df_ent &lt;-<span class="st"> </span>df_ent <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_n</span>(<span class="dv">700</span>)</code></pre>
<p>Intentemos algo simple para empezar: consideramos qué palabras contiene
cada reseña, e intentamos clasificar en base esas palabras. Así que en
primer lugar dividimos cada texto en <em>tokens</em> (pueden ser palabras, o
sucesiones de caracteres o de palabras de tamaño fijo (n-gramas), oraciones, etc.).
En este caso, usamos el paquete <em>tidytext</em>. La función <em>unnest_tokens</em> elimina
signos de puntuación, convierte todo a minúsculas, y separa las palabras:</p>
<p>Vamos a calcular los tokens y ordernarlos por frecuencia. Empezamos
calculando nuestro vocabulario. Supongamos que usamos las 50 palabras más comunes,
y usamos poca regularización:</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">50</span>)
<span class="kw">head</span>(vocabulario)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   palabra  frec
##   &lt;chr&gt;   &lt;int&gt;
## 1 a       12904
## 2 about    1228
## 3 all      1464
## 4 an       2000
## 5 and     12173
## 6 are      2359</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(vocabulario)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   palabra  frec
##   &lt;chr&gt;   &lt;int&gt;
## 1 what     1006
## 2 when     1091
## 3 which    1153
## 4 who      1870
## 5 with     3705
## 6 you      1565</code></pre>

<div class="comentario">
<ul>
<li>Todas las etapas de preprocesamiento deben hacerse en función de los datos de entrenamiento.
En este ejemplo, podríamos cometer el error de usar todos los datos para calcular
el vocabulario.</li>
<li>Nuestras entradas aquí no se ven muy buenas: los términos más comunes son en su mayoría palabras sin significado, de
modo que no esperamos un desempeño muy bueno. En este momento no nos preocupamos
mucho por eso, queremos correr los primeros modelos.
</div></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(glmnet)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="fl">1e-1</span>)</code></pre>
<pre><code>## [1] &quot;Error entrenamiento: 0.31&quot;
## [1] &quot;Error prueba: 0.36&quot;
## [1] &quot;Devianza entrena:1.148&quot;
## [1] &quot;Devianza prueba:1.271&quot;</code></pre>
</div>
<div id="diagnosticos-sesgo-y-varianza" class="section level2">
<h2><span class="header-section-number">6.4</span> Diagnósticos: sesgo y varianza</h2>
<p>Y notamos que</p>
<ul>
<li>El error de entrenamiento no es satisfactorio: está muy por arriba de nuestro objetivo (10%)</li>
<li>Hay algo de brecha entre entrenamiento y prueba, de modo que disminuir varianza puede
ayudar.</li>
</ul>
<p>¿Qué hacer? Nuestro clasificador ni siquiera puede clasificar bien la muestra de entrenamiento,
lo que implica que nuestro modelo tiene sesgo demasiado alto. Controlar la varianza no nos va a ayudar a resolver nuestro problema en este punto. Podemos intentar un modelo
más flexible.</p>

<div class="comentario">
Error de entrenamiento demasiado alto indica que necesitamos probar con modelos
más flexibles (disminuir el sesgo).
</div>

<p>Para disminuir el sesgo podemos:</p>
<ul>
<li>Expander el vocabulario (agregar más entradas)</li>
<li>Crear nuevas entradas a partir de los datos (más informativas)</li>
<li>Usar un método más flexible (como redes neuronales)</li>
<li>Regularizar menos</li>
</ul>
<p>Cosas que no van a funcionar (puede bajar un poco el error de validación, pero
el error de entrenamiento es muy alto):</p>
<ul>
<li>Conseguir más datos de entrenamiento (el error de entrenamiento va a subir, y el de validación va a quedar muy arriba, aunque disminuya)</li>
<li>Regularizar más (misma razón)</li>
<li>Usar un vocabulario más chico, eliminar entradas (misma razón)</li>
</ul>
<p>Por ejemplo, si juntáramos más datos de entrenamiento (con el costo que esto
implica), obtendríamos:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo</span>(df_ent_grande, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="fl">1e-1</span>)</code></pre>
<pre><code>## [1] &quot;Error entrenamiento: 0.31&quot;
## [1] &quot;Error prueba: 0.35&quot;
## [1] &quot;Devianza entrena:1.187&quot;
## [1] &quot;Devianza prueba:1.246&quot;</code></pre>
<p>Vemos que aunque bajó ligeramente el error de prueba, el error es demasiado alto.
Esta estrategia no funcionó con este modelo, y hubiéramos perdido tiempo y dinero
(por duplicar el tamaño de muestra)
sin obtener mejoras apreciables.</p>
<p><strong>Observación</strong>: el error de entrenamiento subió. ¿Puedes explicar eso? Esto sucede
porque típicamente el error para cada caso individual de la muestra original sube, pues la optimización se hace sobre más casos. Es más difícil ajustar los datos de entrenamiento
cuando tenemos más datos.</p>
<p>En lugar de eso, podemos comenzar quitando regularización, por ejemplo</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda =</span><span class="fl">1e-10</span>)</code></pre>
<pre><code>## [1] &quot;Error entrenamiento: 0.29&quot;
## [1] &quot;Error prueba: 0.37&quot;
## [1] &quot;Devianza entrena:1.099&quot;
## [1] &quot;Devianza prueba:1.32&quot;</code></pre>
<p>Y notamos que reducimos un poco el sesgo. Por el momento, seguiremos intentando reducir sesgo. Podemos ahora incluir más variables</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">3000</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda=</span><span class="fl">1e-10</span>)</code></pre>
<pre><code>## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.38&quot;
## [1] &quot;Devianza entrena:0&quot;
## [1] &quot;Devianza prueba:7.66&quot;</code></pre>
<p>El sesgo ya no parece ser un problema: Ahora tenemos
un problema de varianza.</p>

<div class="comentario">
Una brecha grande entre entrenamiento y validación muchas veces indica
sobreajuste (el problema es varianza).
</div>

<p>Podemos regularizar más:</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda=</span><span class="fl">1e-5</span>)</code></pre>
<pre><code>## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.2&quot;
## [1] &quot;Devianza entrena:0&quot;
## [1] &quot;Devianza prueba:1.387&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda=</span><span class="fl">0.01</span>)</code></pre>
<pre><code>## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.18&quot;
## [1] &quot;Devianza entrena:0.021&quot;
## [1] &quot;Devianza prueba:0.797&quot;</code></pre>
<p>Y logramos reducir considerablemente el error y devianza de prueba.</p>
</div>
<div id="refinando-el-pipeline" class="section level2">
<h2><span class="header-section-number">6.5</span> Refinando el pipeline</h2>

<div class="comentario">
Refinar el pipeline para producir mejores entradas, o corridas más rápidas, generalmente
es una buena inversión de tiempo (aunque es mejor no hacerlo prematuramente).
</div>

<p>El error de entrenamiento es satisfactorio todavía, y nos estamos acercando
a nuestro objetivo (intenta regularizar más para verificar que el problema
ahora es sesgo). En este punto, podemos intentar reducir
varianza (reducir error de prueba con algún incremento en error de entrenamiento).</p>
<ul>
<li>Buscar más casos de entrenamiento: si son baratos, esto podría ayudar (aumentar
al doble o 10 veces más).</li>
<li>Redefinir entradas más informativas, para reducir el número de variables pero
al mismo tiempo no aumentar el sesgo.</li>
</ul>
<p>Intentaremos por el momento el segundo camino (reducción de varianza).
Podemos intentar tres cosas:</p>
<ul>
<li>Eliminar los términos que son demasiado frecuentes (son palabras no informativas,
como the, a, he, she, etc.). Esto podría reducir varianza sin afectar mucho el sesgo.</li>
<li>Usar raíces de palabras en lugar de palabras (por ejemplo, transfomar
defect, defects, defective -&gt; defect y boring,bored, bore -&gt; bore, etc.). De esta
manera, controlamos la proliferación de entradas que indican lo mismo y aumentan
varianza - y quizá el sesgo no aumente mucho.</li>
<li>Intentar usar bigramas - esto reduce el sesgo, pero quizá la varianza no aumente mucho.</li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(<span class="st">&quot;stop_words&quot;</span>)
<span class="kw">head</span>(stop_words)</code></pre>
<pre><code>## # A tibble: 6 x 2
##   word      lexicon
##   &lt;chr&gt;     &lt;chr&gt;  
## 1 a         SMART  
## 2 a&#39;s       SMART  
## 3 able      SMART  
## 4 about     SMART  
## 5 above     SMART  
## 6 according SMART</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">100</span>))</code></pre>
<pre><code>## # A tibble: 6 x 2
##   palabra  frec
##   &lt;chr&gt;   &lt;int&gt;
## 1 a       12904
## 2 about    1228
## 3 after     569
## 4 all      1464
## 5 also      704
## 6 an       2000</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">100</span>, <span class="dt">remove_stop =</span> <span class="ot">TRUE</span>))</code></pre>
<pre><code>## # A tibble: 6 x 2
##   palabra   frec
##   &lt;chr&gt;    &lt;int&gt;
## 1 2          179
## 2 acting     224
## 3 action     418
## 4 actor      165
## 5 actors     256
## 6 american   193</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">2000</span>, <span class="dt">remove_stop =</span> <span class="ot">TRUE</span>)
<span class="kw">head</span>(vocabulario <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(frec)),<span class="dv">20</span>)</code></pre>
<pre><code>## # A tibble: 20 x 2
##    palabra     frec
##    &lt;chr&gt;      &lt;int&gt;
##  1 film        2991
##  2 movie       1844
##  3 time         797
##  4 review       788
##  5 story        749
##  6 character    639
##  7 characters   631
##  8 life         527
##  9 films        515
## 10 plot         490
## 11 bad          484
## 12 people       484
## 13 scene        482
## 14 movies       455
## 15 scenes       443
## 16 action       418
## 17 director     413
## 18 love         393
## 19 real         329
## 20 world        323</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(vocabulario <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(frec)),<span class="dv">20</span>)</code></pre>
<pre><code>## # A tibble: 20 x 2
##    palabra        frec
##    &lt;chr&gt;         &lt;int&gt;
##  1 shock            18
##  2 sir              18
##  3 sleep            18
##  4 sole             18
##  5 spot             18
##  6 stays            18
##  7 stereotypical    18
##  8 strip            18
##  9 supergirl        18
## 10 taylor           18
## 11 threat           18
## 12 thrillers        18
## 13 tradition        18
## 14 tree             18
## 15 trial            18
## 16 trio             18
## 17 triumph          18
## 18 visit            18
## 19 warning          18
## 20 werewolf         18</code></pre>
<p>Este vocabulario parece que puede ser más útil. Vamos a tener que ajustar
la regularización de nuevo (y también el número de entradas). Usaremos
ahora validación cruzada para seleccionar modelos. Nota:
este proceso también lo podemos hacer con cv.glmnet de manera más rápida.</p>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent, df_pr, vocabulario, 
                          <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">5</span>,<span class="fl">0.1</span>)))
<span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&#39;./cache_obj/mod_sentiment_1.rds&#39;</span>)
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.201896517994655&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.21&quot;
## [1] &quot;Devianza entrena:0.261&quot;
## [1] &quot;Devianza prueba:0.879&quot;</code></pre>
<p>No estamos mejorando. Podemos intentar con un número diferente de entradas:</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">4000</span>, <span class="dt">remove_stop =</span> <span class="ot">TRUE</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">10</span>,<span class="dv">5</span>,<span class="fl">0.1</span>)))
<span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&#39;./cache_obj/mod_sentiment_2.rds&#39;</span>)
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.49658530379141&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.18&quot;
## [1] &quot;Devianza entrena:0.295&quot;
## [1] &quot;Devianza prueba:0.883&quot;</code></pre>
<p>Y parece que nuestra estrategia no está funcionando muy bien.
Regresamos a nuestro modelo con ridge</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">3000</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))
<span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&#39;./cache_obj/mod_sentiment_3.rds&#39;</span>)
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.110803158362334&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.18&quot;
## [1] &quot;Devianza entrena:0.128&quot;
## [1] &quot;Devianza prueba:0.775&quot;</code></pre>
<p>Podemos intentar aumentar el número de palabras y aumentar también la
regularización</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent, <span class="dv">4000</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))
<span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&#39;./cache_obj/mod_sentiment_4.rds&#39;</span>)
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.22313016014843&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.16&quot;
## [1] &quot;Devianza entrena:0.173&quot;
## [1] &quot;Devianza prueba:0.776&quot;</code></pre>
</div>
<div id="consiguiendo-mas-datos" class="section level2">
<h2><span class="header-section-number">6.6</span> Consiguiendo más datos</h2>

<div class="comentario">
Si nuestro problema es varianza, conseguir más datos de entrenamiento puede
ayudarnos, especialmente si producir estos datos es relativamente barato y rápido.
</div>

<p>Como nuestro principal problema es varianza, podemos mejorar buscando más datos. Supongamos
que hacemos eso en este caso, conseguimos el doble casos de entrenamiento.
En este ejemplo,
podríamos etiquetar más reviews: esto es relativamente barato y rápido</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent_grande, <span class="dv">3000</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent_grande, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))</code></pre>
<pre><code>## Joining, by = &quot;palabra&quot;
## Joining, by = &quot;palabra&quot;</code></pre>
<pre><code>## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.

## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&#39;./cache_obj/mod_sentiment_5.rds&#39;</span>)
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.0907179532894125&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.12&quot;
## [1] &quot;Devianza entrena:0.18&quot;
## [1] &quot;Devianza prueba:0.653&quot;</code></pre>
<p>Y ya casi logramos nuestro objetivo. Podemos intentar con más palabras</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent_grande, <span class="dv">4000</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent_grande, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))</code></pre>
<pre><code>## Joining, by = &quot;palabra&quot;
## Joining, by = &quot;palabra&quot;</code></pre>
<pre><code>## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.

## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&#39;./cache_obj/mod_sentiment_6.rds&#39;</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&#39;./cache_obj/mod_sentiment_6.rds&#39;</span>)
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.0742735782143339&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.12&quot;
## [1] &quot;Devianza entrena:0.127&quot;
## [1] &quot;Devianza prueba:0.621&quot;</code></pre>
<p>Y esto funcionó bien. Subir más la regularización no ayuda mucho (pruébalo).
Parece que el sesgo lo podemos hacer
chico (reducir el error de entrenamiento considerablemente), pero
tenemos un problema más grande con la varianza.</p>
<ul>
<li>Quizá muchas palabras que estamos usando
no tienen qué ver con la calidad de positivo/negativo, y eso induce varianza.</li>
<li>Estos modelos no utilizan la estructura que hay en las reseñas, simplemente
cuentan qué palabras aparecen. Quizá aprovechar esta estructura podemos incluir
variables más informativas que induzcan menos varianza sin aumentar el sesgo.</li>
<li>Podemos conseguir más datos.</li>
</ul>
<p>Obsérvese que:</p>
<ul>
<li>¿Podríamos intentar con una red neuronal totalmente conexa? Probablemente
esto no va a ayudar, pues es un modelo más complejo y nuestro problema es
varianza.</li>
</ul>
</div>
<div id="usar-datos-adicionales" class="section level2">
<h2><span class="header-section-number">6.7</span> Usar datos adicionales</h2>

<div class="comentario">
Considerar fuentes adicionales de datos muchas veces puede ayudar a mejorar
nuestras entradas, lo cual puede tener beneficios en predicción (tanto sesgo como
varianza).
</div>

<p>Intentemos el primer camino. Probamos usar palabras que tengan
afinidad como parte de su significado (positivas y negativas). Estos datos
están incluidos en el paquete <em>tidytext</em>.</p>
<pre class="sourceCode r"><code class="sourceCode r">bing &lt;-<span class="st"> </span><span class="kw">filter</span>(sentiments, lexicon <span class="op">==</span><span class="st"> &#39;bing&#39;</span>)
<span class="kw">tail</span>(bing)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   word      sentiment lexicon score
##   &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;   &lt;int&gt;
## 1 zealous   negative  bing       NA
## 2 zealously negative  bing       NA
## 3 zenith    positive  bing       NA
## 4 zest      positive  bing       NA
## 5 zippy     positive  bing       NA
## 6 zombie    negative  bing       NA</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(vocabulario)</code></pre>
<pre><code>## [1] 4106    2</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent_grande, <span class="dv">8000</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
voc_bing &lt;-<span class="st"> </span>vocabulario <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">inner_join</span>(bing <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rename</span>(<span class="dt">palabra =</span> word))</code></pre>
<pre><code>## Joining, by = &quot;palabra&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">dim</span>(voc_bing)</code></pre>
<pre><code>## [1] 1476    5</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent_grande, df_pr, voc_bing, <span class="dt">alpha=</span><span class="dv">0</span>,
                       <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))</code></pre>
<pre><code>## Joining, by = &quot;palabra&quot;
## Joining, by = &quot;palabra&quot;</code></pre>
<pre><code>## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.

## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.135335283236613&quot;
## [1] &quot;Error entrenamiento: 0.02&quot;
## [1] &quot;Error prueba: 0.18&quot;
## [1] &quot;Devianza entrena:0.399&quot;
## [1] &quot;Devianza prueba:0.775&quot;</code></pre>
<p>Estas variables solas no dan un resultado tan bueno (tenemos tanto sesgo
como varianza altas). Podemos combinar:</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent_grande, <span class="dv">3000</span>, <span class="dt">remove_stop =</span><span class="ot">FALSE</span>)
voc &lt;-<span class="st"> </span><span class="kw">bind_rows</span>(vocabulario, voc_bing <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(palabra, frec)) <span class="op">%&gt;%</span><span class="st"> </span>unique
<span class="kw">dim</span>(voc)</code></pre>
<pre><code>## [1] 4021    2</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent_grande, df_pr, voc, <span class="dt">alpha=</span><span class="dv">0</span>, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))</code></pre>
<pre><code>## Joining, by = &quot;palabra&quot;
## Joining, by = &quot;palabra&quot;</code></pre>
<pre><code>## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.

## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.110803158362334&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.13&quot;
## [1] &quot;Devianza entrena:0.168&quot;
## [1] &quot;Devianza prueba:0.64&quot;</code></pre>
<p>Este camino no se ve mal, pero no hemos logrado mejoras. Aunque quizá valdría la pena
intentar refinar más y ver qué pasa.</p>
</div>
<div id="examen-de-modelo-y-analisis-de-errores" class="section level2">
<h2><span class="header-section-number">6.8</span> Examen de modelo y Análisis de errores</h2>
<p>Ahora podemos ver qué errores estamos cometiendo, y cómo está funcionando el modelo. Busquemos los peores. Corremos el mejor
modelo hasta ahora:</p>
<pre class="sourceCode r"><code class="sourceCode r">vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent_grande, <span class="dv">4000</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent_grande, df_pr, vocabulario, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">2</span>,<span class="fl">0.1</span>)))</code></pre>
<pre><code>## Joining, by = &quot;palabra&quot;
## Joining, by = &quot;palabra&quot;</code></pre>
<pre><code>## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.

## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.0742735782143339&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.12&quot;
## [1] &quot;Devianza entrena:0.127&quot;
## [1] &quot;Devianza prueba:0.621&quot;</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">coeficientes &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_x<span class="op">$</span>mod, <span class="dt">lambda =</span> <span class="st">&#39;lambda.min&#39;</span>, <span class="dt">type =</span> <span class="st">&#39;coefficients&#39;</span>) 
coef_df &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">palabra =</span> <span class="kw">rownames</span>(coeficientes),
                      <span class="dt">coef =</span> coeficientes[,<span class="dv">1</span>])
<span class="kw">arrange</span>(coef_df, coef) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>(<span class="dt">n=</span><span class="dv">20</span>)</code></pre>
<pre><code>## # A tibble: 4,107 x 2
##    palabra        coef
##    &lt;chr&gt;         &lt;dbl&gt;
##  1 (Intercept)  -0.520
##  2 tiresome     -0.318
##  3 sloppy       -0.317
##  4 tedious      -0.313
##  5 designed     -0.287
##  6 profanity    -0.286
##  7 forgot       -0.285
##  8 insulting    -0.273
##  9 redeeming    -0.268
## 10 ludicrous    -0.267
## 11 asleep       -0.264
## 12 embarrassing -0.260
## 13 alas         -0.254
## 14 miserably    -0.252
## 15 lifeless     -0.247
## 16 random       -0.242
## 17 abilities    -0.238
## 18 ridiculous   -0.235
## 19 inept        -0.234
## 20 stupidity    -0.231
## # ... with 4,087 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">arrange</span>(coef_df, <span class="kw">desc</span>(coef)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">print</span>(<span class="dt">n=</span><span class="dv">20</span>)</code></pre>
<pre><code>## # A tibble: 4,107 x 2
##    palabra       coef
##    &lt;chr&gt;        &lt;dbl&gt;
##  1 refreshing   0.306
##  2 beings       0.289
##  3 underneath   0.287
##  4 commanding   0.260
##  5 outstanding  0.245
##  6 marvelous    0.236
##  7 finest       0.230
##  8 identify     0.228
##  9 enjoyment    0.228
## 10 ralph        0.224
## 11 exceptional  0.220
## 12 threatens    0.218
## 13 mature       0.216
## 14 anger        0.216
## 15 luckily      0.214
## 16 enters       0.213
## 17 overall      0.210
## 18 breathtaking 0.208
## 19 popcorn      0.207
## 20 portrait     0.205
## # ... with 4,087 more rows</code></pre>
<p>Y busquemos las diferencias más grandes del la probabilidad ajustada con la
clase observada</p>
<pre class="sourceCode r"><code class="sourceCode r">y &lt;-<span class="st"> </span>mod_x<span class="op">$</span>prueba<span class="op">$</span>y
x &lt;-<span class="st"> </span>mod_x<span class="op">$</span>prueba<span class="op">$</span>x
probs &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_x<span class="op">$</span>mod, <span class="dt">newx =</span> x, <span class="dt">type =</span> <span class="st">&#39;response&#39;</span>, <span class="dt">s =</span><span class="st">&#39;lambda.min&#39;</span>)
df_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">id =</span> <span class="kw">rownames</span>(x), <span class="dt">y=</span>y, <span class="dt">prob =</span> probs[,<span class="dv">1</span>]) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">error =</span> y <span class="op">-</span><span class="st"> </span>prob) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">arrange</span>(<span class="kw">desc</span>(<span class="kw">abs</span>(error)))
df_<span class="dv">1</span></code></pre>
<pre><code>## # A tibble: 425 x 4
##    id        y   prob  error
##    &lt;chr&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1 1508      1 0.0370  0.963
##  2 1461      1 0.0459  0.954
##  3 1490      1 0.0900  0.910
##  4 222       0 0.896  -0.896
##  5 1933      1 0.106   0.894
##  6 1642      1 0.131   0.869
##  7 25        0 0.864  -0.864
##  8 728       0 0.860  -0.860
##  9 1050      1 0.146   0.854
## 10 415       0 0.850  -0.850
## # ... with 415 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(df_pr, id <span class="op">==</span><span class="st"> </span><span class="dv">1461</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(texto) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">str_sub</span>(<span class="dv">1</span>, <span class="dv">500</span>)</code></pre>
<p>[1] “Review deep rising is one of &quot; those &quot; movies . the kind of movie which serves no purpose except to entertain us . it does not ask us to think about important questions like life on other planets or the possibility that there is no god . . . screw that , it says boldly , let’s see some computer generated monsters rip into , decapitate and generally cause irreparable booboos to a bunch of little known actors . heh ! them wacky monsters , gotta love ’em . of course , since we can rent about”</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(df_pr, id <span class="op">==</span><span class="st"> </span><span class="dv">1508</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(texto) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">str_sub</span>(<span class="dv">1</span>, <span class="dv">1000</span>)</code></pre>
<p>[1] “Review capsule : side-splitting comedy that follows its own merciless logic almost through to the end . . . but not without providing a good deal of genuine laughs . most comedies these days have one flaw . they’re not funny . they think they’re funny , but they are devoid of anything really penetrating or dastardly . occasionally a good funny movie sneaks past the deadening hollywood preconceptions of humor and we get a real gem : ruthless people , for instance , which established a microcosm of a setup and played it out to the bitter end . liar liar is built the same way and is just about as funny . this is one of the few movies i’ve seen where i was laughing consistently almost all the way through : instead of a couple of set-pieces that inspired a laugh ( think of the dismal fatal instinct ) , the whole movie works like clockwork . jim carrey playes a high-powered lawyer , to whom lying is as natural as breathing . there is one thing he takes seriously , though : his son”</p>
<p>Estas últimas son reseñas positivas que clasificamos incorrectamente
como negativas. Vemos que en ambas el tono es irónico: por ejemplo,
la primera argumenta que la película es mala, pero disfrutable. Esta
fue etiquetada como una reseña positiva.</p>
<p>Este fenómeno se puede ver como un problema difícil de <strong>sesgo</strong>:
nuestro modelo simple difícilmente podrá captar esta estructura compleja
de ironía.</p>
<p>El problema es diferente para las reseñas negativas.
Veamos algunas de las reseñas negativas peor clasificadas:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(df_pr, id <span class="op">==</span><span class="st"> </span><span class="dv">222</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(texto) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">str_sub</span>(<span class="dv">1</span>, <span class="dv">1000</span>) <span class="co">#negativa</span></code></pre>
<p>[1] “Review it’s probably inevitable that the popular virtual reality genre ( &quot; the matrix , &quot; &quot; existenz &quot; ) would collide with the even more popular serial-killer genre ( &quot; kiss the girls , &quot; &quot; se7en &quot; ) . the result should have been more interesting than &quot; the cell . &quot; as the movie opens , therapist catharine deane ( jennifer lopez ) treats a catatonic boy ( colton james ) by entering his mind through some sort of virtual reality technique that’s never fully explained . after months of therapy sessions in a surreal desert , catharine has no success to report . meanwhile , killer carl stargher ( vincent d’onofrio ) has claimed another victim . his particular hobby is to kidnap young women , keep them in a glass cell overnight , and drown them . he takes the corpse and soaks it in bleach , then suspends himself over the body and jerks off while watching a video tape of the drowning . although carl’s been doing this for awhile , he’s recently become sloppy , and fbi agent peter nova”</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">filter</span>(df_pr, id <span class="op">==</span><span class="st"> </span><span class="dv">728</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(texto) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">str_sub</span>(<span class="dv">1</span>, <span class="dv">1000</span>) <span class="co">#negativa</span></code></pre>
<p>[1] “Review girl 6 is , in a word , a mess . i was never able to determine what spike lee was trying to accomplish with this film . there was no sense of where the film was going , or any kind of coherent narrative . if there was a point to the film , i missed it . girl 6 , by the way , is the way theresa randle’s character is addressed in the phone sex workplace ; all the girls are known by their numbers . the plot , such as it is : theresa randle is a struggling n . y . actress , and eventually takes a job as a phone-sex operator . she begins to lose contact with reality , as her job consumes her . also , she must deal with the advances of her ex-husband ( isiah washington ) . he is an ex- con thief , and she tries to keep him away , while at the same time , it’s clear that she still harbors feelings for him . her neighbor , jimmy ( spike lee ) functions as the observer ; mediating between the ex- husband and girl 6 . he also functions as a point of stability , as he watches he”</p>
<p>No está totalmente claro por qué nos equivocamos en estas dos reseñas.
Podemos hacer un examen más cuidadoso de la construcción del predictor,
obteniendo los coeficientes <span class="math inline">\(\beta\)</span> y el vector <span class="math inline">\(x\)</span> con los que se construyen
el predictor:</p>
<pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span><span class="kw">coef</span>(mod_x<span class="op">$</span>mod) <span class="op">%&gt;%</span><span class="st"> </span>as.numeric
nombres &lt;-<span class="st"> </span><span class="kw">rownames</span>(x)
<span class="kw">head</span>(<span class="kw">sort</span>(x[nombres <span class="op">==</span><span class="st"> &quot;222&quot;</span>, ], <span class="dt">decreasing =</span> <span class="ot">TRUE</span>), <span class="dv">100</span>)</code></pre>
<pre><code>##       the        in        of        to         a       and        is 
##        52        21        17        17        16        14        10 
##      cell      mind      have      that      this        as      been 
##         9         7         5         5         5         4         4 
##       has       his     horse    killer      more       she      than 
##         4         4         4         4         4         4         4 
##      with       all        an   another        by     could      fast 
##         4         3         3         3         3         3         3 
##       for     glass       out     peter     seems    should     video 
##         3         3         3         3         3         3         3 
##     after        at    before       boy       can  computer developed 
##         2         2         2         2         2         2         2 
##      find      from generated     genre        go        he       him 
##         2         2         2         2         2         2         2 
##         i      into        it      it&#39;s      keep      like     movie 
##         2         2         2         2         2         2         2 
##        no       not       off        on       one        or       own 
##         2         2         2         2         2         2         2 
##   popular   promise   reality    really      room   surreal      them 
##         2         2         2         2         2         2         2 
##      time  universe   virtual      well    acting     agent  although 
##         2         2         2         2         1         1         1 
##     apart    attack        be   because    become     begin      best 
##         1         1         1         1         1         1         1 
##   bizarre      body    bottom    brings       but     catch    center 
##         1         1         1         1         1         1         1 
## character   closing  costumes   creates      dark  darkness       day 
##         1         1         1         1         1         1         1 
##     depth    desert 
##         1         1</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">predictor &lt;-<span class="st"> </span>beta <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, x[nombres<span class="op">==</span><span class="st">&quot;222&quot;</span>,])  <span class="co"># beta*x</span>
<span class="kw">sum</span>(predictor)</code></pre>
<pre><code>## [1] 1.437326</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sort</span>(predictor[predictor <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>]) <span class="op">%&gt;%</span><span class="st"> </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td align="right">-0.5202993</td>
</tr>
<tr class="even">
<td>sloppy</td>
<td align="right">-0.3172574</td>
</tr>
<tr class="odd">
<td>promise</td>
<td align="right">-0.2760900</td>
</tr>
<tr class="even">
<td>video</td>
<td align="right">-0.1501897</td>
</tr>
<tr class="odd">
<td>dull</td>
<td align="right">-0.1331210</td>
</tr>
<tr class="even">
<td>catch</td>
<td align="right">-0.1169287</td>
</tr>
<tr class="odd">
<td>should</td>
<td align="right">-0.1159415</td>
</tr>
<tr class="even">
<td>suffers</td>
<td align="right">-0.1128175</td>
</tr>
<tr class="odd">
<td>trapped</td>
<td align="right">-0.1111792</td>
</tr>
<tr class="even">
<td>could</td>
<td align="right">-0.1011409</td>
</tr>
<tr class="odd">
<td>pulling</td>
<td align="right">-0.1003304</td>
</tr>
<tr class="even">
<td>bottom</td>
<td align="right">-0.0939438</td>
</tr>
<tr class="odd">
<td>fast</td>
<td align="right">-0.0911754</td>
</tr>
<tr class="even">
<td>been</td>
<td align="right">-0.0908088</td>
</tr>
<tr class="odd">
<td>save</td>
<td align="right">-0.0876571</td>
</tr>
<tr class="even">
<td>explained</td>
<td align="right">-0.0808605</td>
</tr>
<tr class="odd">
<td>have</td>
<td align="right">-0.0796675</td>
</tr>
<tr class="even">
<td>mtv</td>
<td align="right">-0.0714969</td>
</tr>
<tr class="odd">
<td>talking</td>
<td align="right">-0.0639503</td>
</tr>
<tr class="even">
<td>kidnapped</td>
<td align="right">-0.0600789</td>
</tr>
<tr class="odd">
<td>water</td>
<td align="right">-0.0600346</td>
</tr>
<tr class="even">
<td>vince</td>
<td align="right">-0.0571824</td>
</tr>
<tr class="odd">
<td>begin</td>
<td align="right">-0.0547786</td>
</tr>
<tr class="even">
<td>jennifer</td>
<td align="right">-0.0528719</td>
</tr>
<tr class="odd">
<td>virtual</td>
<td align="right">-0.0519579</td>
</tr>
<tr class="even">
<td>twisted</td>
<td align="right">-0.0508402</td>
</tr>
<tr class="odd">
<td>center</td>
<td align="right">-0.0505813</td>
</tr>
<tr class="even">
<td>provided</td>
<td align="right">-0.0492090</td>
</tr>
<tr class="odd">
<td>psycho</td>
<td align="right">-0.0489186</td>
</tr>
<tr class="even">
<td>off</td>
<td align="right">-0.0482361</td>
</tr>
<tr class="odd">
<td>recently</td>
<td align="right">-0.0482342</td>
</tr>
<tr class="even">
<td>result</td>
<td align="right">-0.0476513</td>
</tr>
<tr class="odd">
<td>women</td>
<td align="right">-0.0472648</td>
</tr>
<tr class="even">
<td>point</td>
<td align="right">-0.0472133</td>
</tr>
<tr class="odd">
<td>within</td>
<td align="right">-0.0458799</td>
</tr>
<tr class="even">
<td>forward</td>
<td align="right">-0.0456438</td>
</tr>
<tr class="odd">
<td>exercise</td>
<td align="right">-0.0452252</td>
</tr>
<tr class="even">
<td>no</td>
<td align="right">-0.0410514</td>
</tr>
<tr class="odd">
<td>technique</td>
<td align="right">-0.0405685</td>
</tr>
<tr class="even">
<td>director</td>
<td align="right">-0.0358213</td>
</tr>
<tr class="odd">
<td>focus</td>
<td align="right">-0.0351205</td>
</tr>
<tr class="even">
<td>acting</td>
<td align="right">-0.0345010</td>
</tr>
<tr class="odd">
<td>interesting</td>
<td align="right">-0.0334681</td>
</tr>
<tr class="even">
<td>style</td>
<td align="right">-0.0332168</td>
</tr>
<tr class="odd">
<td>thomas</td>
<td align="right">-0.0322831</td>
</tr>
<tr class="even">
<td>kept</td>
<td align="right">-0.0316398</td>
</tr>
<tr class="odd">
<td>hardly</td>
<td align="right">-0.0309470</td>
</tr>
<tr class="even">
<td>another</td>
<td align="right">-0.0307244</td>
</tr>
<tr class="odd">
<td>attack</td>
<td align="right">-0.0302758</td>
</tr>
<tr class="even">
<td>explored</td>
<td align="right">-0.0292849</td>
</tr>
<tr class="odd">
<td>then</td>
<td align="right">-0.0292602</td>
</tr>
<tr class="even">
<td>or</td>
<td align="right">-0.0290317</td>
</tr>
<tr class="odd">
<td>victim</td>
<td align="right">-0.0276020</td>
</tr>
<tr class="even">
<td>fill</td>
<td align="right">-0.0267911</td>
</tr>
<tr class="odd">
<td>hope</td>
<td align="right">-0.0266701</td>
</tr>
<tr class="even">
<td>even</td>
<td align="right">-0.0250174</td>
</tr>
<tr class="odd">
<td>enough</td>
<td align="right">-0.0249711</td>
</tr>
<tr class="even">
<td>woman</td>
<td align="right">-0.0244227</td>
</tr>
<tr class="odd">
<td>fall</td>
<td align="right">-0.0234435</td>
</tr>
<tr class="even">
<td>apart</td>
<td align="right">-0.0233941</td>
</tr>
<tr class="odd">
<td>out</td>
<td align="right">-0.0230786</td>
</tr>
<tr class="even">
<td>this</td>
<td align="right">-0.0196350</td>
</tr>
<tr class="odd">
<td>to</td>
<td align="right">-0.0184586</td>
</tr>
<tr class="even">
<td>premise</td>
<td align="right">-0.0180929</td>
</tr>
<tr class="odd">
<td>she’s</td>
<td align="right">-0.0179955</td>
</tr>
<tr class="even">
<td>killer</td>
<td align="right">-0.0173511</td>
</tr>
<tr class="odd">
<td>left</td>
<td align="right">-0.0173469</td>
</tr>
<tr class="even">
<td>development</td>
<td align="right">-0.0172162</td>
</tr>
<tr class="odd">
<td>how</td>
<td align="right">-0.0165661</td>
</tr>
<tr class="even">
<td>into</td>
<td align="right">-0.0162641</td>
</tr>
<tr class="odd">
<td>at</td>
<td align="right">-0.0153004</td>
</tr>
<tr class="even">
<td>discover</td>
<td align="right">-0.0150697</td>
</tr>
<tr class="odd">
<td>them</td>
<td align="right">-0.0133533</td>
</tr>
<tr class="even">
<td>would</td>
<td align="right">-0.0129188</td>
</tr>
<tr class="odd">
<td>james</td>
<td align="right">-0.0124600</td>
</tr>
<tr class="even">
<td>on</td>
<td align="right">-0.0124260</td>
</tr>
<tr class="odd">
<td>where</td>
<td align="right">-0.0121713</td>
</tr>
<tr class="even">
<td>sort</td>
<td align="right">-0.0121419</td>
</tr>
<tr class="odd">
<td>much</td>
<td align="right">-0.0114857</td>
</tr>
<tr class="even">
<td>costumes</td>
<td align="right">-0.0111567</td>
</tr>
<tr class="odd">
<td>turns</td>
<td align="right">-0.0110439</td>
</tr>
<tr class="even">
<td>so</td>
<td align="right">-0.0108164</td>
</tr>
<tr class="odd">
<td>movie</td>
<td align="right">-0.0108057</td>
</tr>
<tr class="even">
<td>end</td>
<td align="right">-0.0107060</td>
</tr>
<tr class="odd">
<td>review</td>
<td align="right">-0.0105867</td>
</tr>
<tr class="even">
<td>be</td>
<td align="right">-0.0104210</td>
</tr>
<tr class="odd">
<td>don’t</td>
<td align="right">-0.0102111</td>
</tr>
<tr class="even">
<td>had</td>
<td align="right">-0.0100659</td>
</tr>
<tr class="odd">
<td>like</td>
<td align="right">-0.0100186</td>
</tr>
<tr class="even">
<td>because</td>
<td align="right">-0.0099953</td>
</tr>
<tr class="odd">
<td>seems</td>
<td align="right">-0.0096492</td>
</tr>
<tr class="even">
<td>girls</td>
<td align="right">-0.0096262</td>
</tr>
<tr class="odd">
<td>tape</td>
<td align="right">-0.0089222</td>
</tr>
<tr class="even">
<td>through</td>
<td align="right">-0.0089024</td>
</tr>
<tr class="odd">
<td>character</td>
<td align="right">-0.0087373</td>
</tr>
<tr class="even">
<td>all</td>
<td align="right">-0.0081031</td>
</tr>
<tr class="odd">
<td>room</td>
<td align="right">-0.0078808</td>
</tr>
<tr class="even">
<td>long</td>
<td align="right">-0.0074416</td>
</tr>
<tr class="odd">
<td>get</td>
<td align="right">-0.0068225</td>
</tr>
<tr class="even">
<td>some</td>
<td align="right">-0.0054598</td>
</tr>
<tr class="odd">
<td>thought</td>
<td align="right">-0.0052326</td>
</tr>
<tr class="even">
<td>fbi</td>
<td align="right">-0.0052078</td>
</tr>
<tr class="odd">
<td>bizarre</td>
<td align="right">-0.0050159</td>
</tr>
<tr class="even">
<td>opportunity</td>
<td align="right">-0.0048392</td>
</tr>
<tr class="odd">
<td>house</td>
<td align="right">-0.0047232</td>
</tr>
<tr class="even">
<td>forty</td>
<td align="right">-0.0037017</td>
</tr>
<tr class="odd">
<td>after</td>
<td align="right">-0.0036686</td>
</tr>
<tr class="even">
<td>minds</td>
<td align="right">-0.0035547</td>
</tr>
<tr class="odd">
<td>doing</td>
<td align="right">-0.0035518</td>
</tr>
<tr class="even">
<td>my</td>
<td align="right">-0.0030584</td>
</tr>
<tr class="odd">
<td>hours</td>
<td align="right">-0.0030343</td>
</tr>
<tr class="even">
<td>scene</td>
<td align="right">-0.0029061</td>
</tr>
<tr class="odd">
<td>girl</td>
<td align="right">-0.0026162</td>
</tr>
<tr class="even">
<td>i</td>
<td align="right">-0.0024423</td>
</tr>
<tr class="odd">
<td>psychotic</td>
<td align="right">-0.0014919</td>
</tr>
<tr class="even">
<td>next</td>
<td align="right">-0.0013199</td>
</tr>
<tr class="odd">
<td>singer</td>
<td align="right">-0.0012470</td>
</tr>
<tr class="even">
<td>that</td>
<td align="right">-0.0007843</td>
</tr>
<tr class="odd">
<td>watching</td>
<td align="right">-0.0000763</td>
</tr>
<tr class="even">
<td>but</td>
<td align="right">0.0001236</td>
</tr>
<tr class="odd">
<td>standing</td>
<td align="right">0.0002228</td>
</tr>
<tr class="even">
<td>himself</td>
<td align="right">0.0003255</td>
</tr>
<tr class="odd">
<td>pieces</td>
<td align="right">0.0003999</td>
</tr>
<tr class="even">
<td>popular</td>
<td align="right">0.0007742</td>
</tr>
<tr class="odd">
<td>its</td>
<td align="right">0.0017265</td>
</tr>
<tr class="even">
<td>she</td>
<td align="right">0.0022264</td>
</tr>
<tr class="odd">
<td>can</td>
<td align="right">0.0025564</td>
</tr>
<tr class="even">
<td>think</td>
<td align="right">0.0025935</td>
</tr>
<tr class="odd">
<td>they</td>
<td align="right">0.0027960</td>
</tr>
<tr class="even">
<td>over</td>
<td align="right">0.0029586</td>
</tr>
<tr class="odd">
<td>part</td>
<td align="right">0.0036062</td>
</tr>
<tr class="even">
<td>personality</td>
<td align="right">0.0037015</td>
</tr>
<tr class="odd">
<td>he’s</td>
<td align="right">0.0039892</td>
</tr>
<tr class="even">
<td>one</td>
<td align="right">0.0040252</td>
</tr>
<tr class="odd">
<td>existenz</td>
<td align="right">0.0040481</td>
</tr>
<tr class="even">
<td>never</td>
<td align="right">0.0042116</td>
</tr>
<tr class="odd">
<td>it</td>
<td align="right">0.0042247</td>
</tr>
<tr class="even">
<td>substance</td>
<td align="right">0.0042565</td>
</tr>
<tr class="odd">
<td>that’s</td>
<td align="right">0.0045095</td>
</tr>
<tr class="even">
<td>kiss</td>
<td align="right">0.0054557</td>
</tr>
<tr class="odd">
<td>an</td>
<td align="right">0.0056231</td>
</tr>
<tr class="even">
<td>known</td>
<td align="right">0.0057068</td>
</tr>
<tr class="odd">
<td>really</td>
<td align="right">0.0057512</td>
</tr>
<tr class="even">
<td>element</td>
<td align="right">0.0058064</td>
</tr>
<tr class="odd">
<td>not</td>
<td align="right">0.0059663</td>
</tr>
<tr class="even">
<td>place</td>
<td align="right">0.0060158</td>
</tr>
<tr class="odd">
<td>horse</td>
<td align="right">0.0070165</td>
</tr>
<tr class="even">
<td>go</td>
<td align="right">0.0072319</td>
</tr>
<tr class="odd">
<td>without</td>
<td align="right">0.0073540</td>
</tr>
<tr class="even">
<td>time</td>
<td align="right">0.0078486</td>
</tr>
<tr class="odd">
<td>however</td>
<td align="right">0.0078614</td>
</tr>
<tr class="even">
<td>for</td>
<td align="right">0.0081780</td>
</tr>
<tr class="odd">
<td>their</td>
<td align="right">0.0088714</td>
</tr>
<tr class="even">
<td>first</td>
<td align="right">0.0093176</td>
</tr>
<tr class="odd">
<td>closing</td>
<td align="right">0.0100908</td>
</tr>
<tr class="even">
<td>serial</td>
<td align="right">0.0104635</td>
</tr>
<tr class="odd">
<td>of</td>
<td align="right">0.0108933</td>
</tr>
<tr class="even">
<td>rather</td>
<td align="right">0.0113546</td>
</tr>
<tr class="odd">
<td>opens</td>
<td align="right">0.0114884</td>
</tr>
<tr class="even">
<td>him</td>
<td align="right">0.0115767</td>
</tr>
<tr class="odd">
<td>michael</td>
<td align="right">0.0115885</td>
</tr>
<tr class="even">
<td>he</td>
<td align="right">0.0116202</td>
</tr>
<tr class="odd">
<td>living</td>
<td align="right">0.0126425</td>
</tr>
<tr class="even">
<td>fate</td>
<td align="right">0.0126835</td>
</tr>
<tr class="odd">
<td>meanwhile</td>
<td align="right">0.0129676</td>
</tr>
<tr class="even">
<td>though</td>
<td align="right">0.0129746</td>
</tr>
<tr class="odd">
<td>his</td>
<td align="right">0.0136937</td>
</tr>
<tr class="even">
<td>slow</td>
<td align="right">0.0141722</td>
</tr>
<tr class="odd">
<td>peter</td>
<td align="right">0.0144512</td>
</tr>
<tr class="even">
<td>vincent</td>
<td align="right">0.0148222</td>
</tr>
<tr class="odd">
<td>young</td>
<td align="right">0.0150035</td>
</tr>
<tr class="even">
<td>day</td>
<td align="right">0.0150646</td>
</tr>
<tr class="odd">
<td>does</td>
<td align="right">0.0152965</td>
</tr>
<tr class="even">
<td>it’s</td>
<td align="right">0.0154444</td>
</tr>
<tr class="odd">
<td>by</td>
<td align="right">0.0156547</td>
</tr>
<tr class="even">
<td>depth</td>
<td align="right">0.0156696</td>
</tr>
<tr class="odd">
<td>importance</td>
<td align="right">0.0159429</td>
</tr>
<tr class="even">
<td>while</td>
<td align="right">0.0163877</td>
</tr>
<tr class="odd">
<td>will</td>
<td align="right">0.0164211</td>
</tr>
<tr class="even">
<td>world</td>
<td align="right">0.0165024</td>
</tr>
<tr class="odd">
<td>has</td>
<td align="right">0.0168118</td>
</tr>
<tr class="even">
<td>particular</td>
<td align="right">0.0174925</td>
</tr>
<tr class="odd">
<td>more</td>
<td align="right">0.0176460</td>
</tr>
<tr class="even">
<td>a</td>
<td align="right">0.0189342</td>
</tr>
<tr class="odd">
<td>effect</td>
<td align="right">0.0193250</td>
</tr>
<tr class="even">
<td>agent</td>
<td align="right">0.0199253</td>
</tr>
<tr class="odd">
<td>creates</td>
<td align="right">0.0209483</td>
</tr>
<tr class="even">
<td>leaves</td>
<td align="right">0.0215036</td>
</tr>
<tr class="odd">
<td>see</td>
<td align="right">0.0217573</td>
</tr>
<tr class="even">
<td>with</td>
<td align="right">0.0231662</td>
</tr>
<tr class="odd">
<td>role</td>
<td align="right">0.0232334</td>
</tr>
<tr class="even">
<td>from</td>
<td align="right">0.0234281</td>
</tr>
<tr class="odd">
<td>body</td>
<td align="right">0.0234823</td>
</tr>
<tr class="even">
<td>than</td>
<td align="right">0.0237709</td>
</tr>
<tr class="odd">
<td>probably</td>
<td align="right">0.0239042</td>
</tr>
<tr class="even">
<td>developed</td>
<td align="right">0.0240430</td>
</tr>
<tr class="odd">
<td>elaborate</td>
<td align="right">0.0243947</td>
</tr>
<tr class="even">
<td>suddenly</td>
<td align="right">0.0247735</td>
</tr>
<tr class="odd">
<td>logic</td>
<td align="right">0.0249532</td>
</tr>
<tr class="even">
<td>most</td>
<td align="right">0.0265502</td>
</tr>
<tr class="odd">
<td>line</td>
<td align="right">0.0271946</td>
</tr>
<tr class="even">
<td>music</td>
<td align="right">0.0273477</td>
</tr>
<tr class="odd">
<td>as</td>
<td align="right">0.0278241</td>
</tr>
<tr class="even">
<td>still</td>
<td align="right">0.0292254</td>
</tr>
<tr class="odd">
<td>months</td>
<td align="right">0.0294009</td>
</tr>
<tr class="even">
<td>shows</td>
<td align="right">0.0296746</td>
</tr>
<tr class="odd">
<td>psychological</td>
<td align="right">0.0313499</td>
</tr>
<tr class="even">
<td>head</td>
<td align="right">0.0321303</td>
</tr>
<tr class="odd">
<td>boy</td>
<td align="right">0.0340266</td>
</tr>
<tr class="even">
<td>darkness</td>
<td align="right">0.0342972</td>
</tr>
<tr class="odd">
<td>become</td>
<td align="right">0.0346070</td>
</tr>
<tr class="even">
<td>very</td>
<td align="right">0.0348716</td>
</tr>
<tr class="odd">
<td>father</td>
<td align="right">0.0352316</td>
</tr>
<tr class="even">
<td>although</td>
<td align="right">0.0354252</td>
</tr>
<tr class="odd">
<td>sound</td>
<td align="right">0.0358830</td>
</tr>
<tr class="even">
<td>finds</td>
<td align="right">0.0375483</td>
</tr>
<tr class="odd">
<td>matrix</td>
<td align="right">0.0377628</td>
</tr>
<tr class="even">
<td>particularly</td>
<td align="right">0.0390302</td>
</tr>
<tr class="odd">
<td>brings</td>
<td align="right">0.0400255</td>
</tr>
<tr class="even">
<td>success</td>
<td align="right">0.0403674</td>
</tr>
<tr class="odd">
<td>before</td>
<td align="right">0.0412008</td>
</tr>
<tr class="even">
<td>directing</td>
<td align="right">0.0413102</td>
</tr>
<tr class="odd">
<td>viewer</td>
<td align="right">0.0421106</td>
</tr>
<tr class="even">
<td>sidney</td>
<td align="right">0.0425819</td>
</tr>
<tr class="odd">
<td>best</td>
<td align="right">0.0432040</td>
</tr>
<tr class="even">
<td>the</td>
<td align="right">0.0439582</td>
</tr>
<tr class="odd">
<td>is</td>
<td align="right">0.0443391</td>
</tr>
<tr class="even">
<td>takes</td>
<td align="right">0.0448486</td>
</tr>
<tr class="odd">
<td>dark</td>
<td align="right">0.0449853</td>
</tr>
<tr class="even">
<td>inside</td>
<td align="right">0.0476245</td>
</tr>
<tr class="odd">
<td>separate</td>
<td align="right">0.0479765</td>
</tr>
<tr class="even">
<td>in</td>
<td align="right">0.0487678</td>
</tr>
<tr class="odd">
<td>find</td>
<td align="right">0.0500643</td>
</tr>
<tr class="even">
<td>great</td>
<td align="right">0.0523020</td>
</tr>
<tr class="odd">
<td>together</td>
<td align="right">0.0581238</td>
</tr>
<tr class="even">
<td>computer</td>
<td align="right">0.0586483</td>
</tr>
<tr class="odd">
<td>genre</td>
<td align="right">0.0593194</td>
</tr>
<tr class="even">
<td>own</td>
<td align="right">0.0625957</td>
</tr>
<tr class="odd">
<td>reality</td>
<td align="right">0.0627538</td>
</tr>
<tr class="even">
<td>disturbing</td>
<td align="right">0.0636809</td>
</tr>
<tr class="odd">
<td>keep</td>
<td align="right">0.0642032</td>
</tr>
<tr class="even">
<td>and</td>
<td align="right">0.0648463</td>
</tr>
<tr class="odd">
<td>offer</td>
<td align="right">0.0736850</td>
</tr>
<tr class="even">
<td>strangely</td>
<td align="right">0.0743557</td>
</tr>
<tr class="odd">
<td>inevitable</td>
<td align="right">0.0759366</td>
</tr>
<tr class="even">
<td>fully</td>
<td align="right">0.0789776</td>
</tr>
<tr class="odd">
<td>jake</td>
<td align="right">0.0797114</td>
</tr>
<tr class="even">
<td>frightened</td>
<td align="right">0.0824091</td>
</tr>
<tr class="odd">
<td>provoking</td>
<td align="right">0.0846733</td>
</tr>
<tr class="even">
<td>well</td>
<td align="right">0.0880060</td>
</tr>
<tr class="odd">
<td>desert</td>
<td align="right">0.0899893</td>
</tr>
<tr class="even">
<td>treats</td>
<td align="right">0.0985579</td>
</tr>
<tr class="odd">
<td>losing</td>
<td align="right">0.0990976</td>
</tr>
<tr class="even">
<td>religion</td>
<td align="right">0.1298072</td>
</tr>
<tr class="odd">
<td>generated</td>
<td align="right">0.1304031</td>
</tr>
<tr class="even">
<td>universe</td>
<td align="right">0.1438618</td>
</tr>
<tr class="odd">
<td>madness</td>
<td align="right">0.1580777</td>
</tr>
<tr class="even">
<td>sharp</td>
<td align="right">0.1604157</td>
</tr>
<tr class="odd">
<td>enters</td>
<td align="right">0.2131698</td>
</tr>
<tr class="even">
<td>surreal</td>
<td align="right">0.2394179</td>
</tr>
<tr class="odd">
<td>mind</td>
<td align="right">0.2858087</td>
</tr>
<tr class="even">
<td>glass</td>
<td align="right">0.4687988</td>
</tr>
<tr class="odd">
<td>cell</td>
<td align="right">0.7086423</td>
</tr>
</tbody>
</table>
<pre class="sourceCode r"><code class="sourceCode r">beta &lt;-<span class="st"> </span><span class="kw">coef</span>(mod_x<span class="op">$</span>mod) <span class="op">%&gt;%</span><span class="st"> </span>as.numeric
nombres &lt;-<span class="st"> </span><span class="kw">rownames</span>(x)
predictor &lt;-<span class="st"> </span>beta <span class="op">*</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, x[nombres<span class="op">==</span><span class="st">&quot;728&quot;</span>,])  <span class="co"># beta*x</span>
<span class="kw">sum</span>(predictor)</code></pre>
<pre><code>## [1] 1.177288</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sort</span>(predictor[predictor <span class="op">!=</span><span class="st"> </span><span class="dv">0</span>]) <span class="op">%&gt;%</span><span class="st"> </span>knitr<span class="op">::</span><span class="kw">kable</span>()</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">x</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td align="right">-0.5202993</td>
</tr>
<tr class="even">
<td>mess</td>
<td align="right">-0.2022808</td>
</tr>
<tr class="odd">
<td>impression</td>
<td align="right">-0.1506333</td>
</tr>
<tr class="even">
<td>grade</td>
<td align="right">-0.1503932</td>
</tr>
<tr class="odd">
<td>struggling</td>
<td align="right">-0.1301878</td>
</tr>
<tr class="even">
<td>there</td>
<td align="right">-0.1228140</td>
</tr>
<tr class="odd">
<td>loud</td>
<td align="right">-0.1042231</td>
</tr>
<tr class="even">
<td>point</td>
<td align="right">-0.0944266</td>
</tr>
<tr class="odd">
<td>onscreen</td>
<td align="right">-0.0910190</td>
</tr>
<tr class="even">
<td>nothing</td>
<td align="right">-0.0838901</td>
</tr>
<tr class="odd">
<td>tries</td>
<td align="right">-0.0810926</td>
</tr>
<tr class="even">
<td>stuck</td>
<td align="right">-0.0777950</td>
</tr>
<tr class="odd">
<td>seemed</td>
<td align="right">-0.0768979</td>
</tr>
<tr class="even">
<td>numbers</td>
<td align="right">-0.0716475</td>
</tr>
<tr class="odd">
<td>bad</td>
<td align="right">-0.0704995</td>
</tr>
<tr class="even">
<td>confused</td>
<td align="right">-0.0645791</td>
</tr>
<tr class="odd">
<td>con</td>
<td align="right">-0.0619718</td>
</tr>
<tr class="even">
<td>missed</td>
<td align="right">-0.0598450</td>
</tr>
<tr class="odd">
<td>sex</td>
<td align="right">-0.0582862</td>
</tr>
<tr class="even">
<td>wasn’t</td>
<td align="right">-0.0534325</td>
</tr>
<tr class="odd">
<td>even</td>
<td align="right">-0.0500348</td>
</tr>
<tr class="even">
<td>phone</td>
<td align="right">-0.0499893</td>
</tr>
<tr class="odd">
<td>plot</td>
<td align="right">-0.0488645</td>
</tr>
<tr class="even">
<td>women</td>
<td align="right">-0.0472648</td>
</tr>
<tr class="odd">
<td>lose</td>
<td align="right">-0.0471523</td>
</tr>
<tr class="even">
<td>stone</td>
<td align="right">-0.0467291</td>
</tr>
<tr class="odd">
<td>middle</td>
<td align="right">-0.0416681</td>
</tr>
<tr class="even">
<td>lee</td>
<td align="right">-0.0414526</td>
</tr>
<tr class="odd">
<td>trying</td>
<td align="right">-0.0402636</td>
</tr>
<tr class="even">
<td>should</td>
<td align="right">-0.0386472</td>
</tr>
<tr class="odd">
<td>was</td>
<td align="right">-0.0380697</td>
</tr>
<tr class="even">
<td>any</td>
<td align="right">-0.0374969</td>
</tr>
<tr class="odd">
<td>sequences</td>
<td align="right">-0.0373245</td>
</tr>
<tr class="even">
<td>only</td>
<td align="right">-0.0365272</td>
</tr>
<tr class="odd">
<td>buddy</td>
<td align="right">-0.0357364</td>
</tr>
<tr class="even">
<td>he’d</td>
<td align="right">-0.0345297</td>
</tr>
<tr class="odd">
<td>acting</td>
<td align="right">-0.0345010</td>
</tr>
<tr class="even">
<td>interesting</td>
<td align="right">-0.0334681</td>
</tr>
<tr class="odd">
<td>i’d</td>
<td align="right">-0.0325935</td>
</tr>
<tr class="even">
<td>kept</td>
<td align="right">-0.0316398</td>
</tr>
<tr class="odd">
<td>be</td>
<td align="right">-0.0312630</td>
</tr>
<tr class="even">
<td>if</td>
<td align="right">-0.0309857</td>
</tr>
<tr class="odd">
<td>fan</td>
<td align="right">-0.0303648</td>
</tr>
<tr class="even">
<td>becomes</td>
<td align="right">-0.0292044</td>
</tr>
<tr class="odd">
<td>or</td>
<td align="right">-0.0290317</td>
</tr>
<tr class="even">
<td>idea</td>
<td align="right">-0.0261244</td>
</tr>
<tr class="odd">
<td>die</td>
<td align="right">-0.0243408</td>
</tr>
<tr class="even">
<td>such</td>
<td align="right">-0.0222690</td>
</tr>
<tr class="odd">
<td>i’m</td>
<td align="right">-0.0215163</td>
</tr>
<tr class="even">
<td>actress</td>
<td align="right">-0.0205609</td>
</tr>
<tr class="odd">
<td>no</td>
<td align="right">-0.0205257</td>
</tr>
<tr class="even">
<td>hard</td>
<td align="right">-0.0196748</td>
</tr>
<tr class="odd">
<td>character</td>
<td align="right">-0.0174746</td>
</tr>
<tr class="even">
<td>some</td>
<td align="right">-0.0163793</td>
</tr>
<tr class="odd">
<td>away</td>
<td align="right">-0.0162291</td>
</tr>
<tr class="even">
<td>have</td>
<td align="right">-0.0159335</td>
</tr>
<tr class="odd">
<td>girl</td>
<td align="right">-0.0156973</td>
</tr>
<tr class="even">
<td>happens</td>
<td align="right">-0.0150229</td>
</tr>
<tr class="odd">
<td>make</td>
<td align="right">-0.0141744</td>
</tr>
<tr class="even">
<td>to</td>
<td align="right">-0.0130296</td>
</tr>
<tr class="odd">
<td>background</td>
<td align="right">-0.0129527</td>
</tr>
<tr class="even">
<td>where</td>
<td align="right">-0.0121713</td>
</tr>
<tr class="odd">
<td>this</td>
<td align="right">-0.0117810</td>
</tr>
<tr class="even">
<td>setup</td>
<td align="right">-0.0115887</td>
</tr>
<tr class="odd">
<td>much</td>
<td align="right">-0.0114857</td>
</tr>
<tr class="even">
<td>d</td>
<td align="right">-0.0113921</td>
</tr>
<tr class="odd">
<td>made</td>
<td align="right">-0.0112626</td>
</tr>
<tr class="even">
<td>gave</td>
<td align="right">-0.0109618</td>
</tr>
<tr class="odd">
<td>going</td>
<td align="right">-0.0106231</td>
</tr>
<tr class="even">
<td>might</td>
<td align="right">-0.0105983</td>
</tr>
<tr class="odd">
<td>review</td>
<td align="right">-0.0105867</td>
</tr>
<tr class="even">
<td>scenes</td>
<td align="right">-0.0100794</td>
</tr>
<tr class="odd">
<td>had</td>
<td align="right">-0.0100659</td>
</tr>
<tr class="even">
<td>like</td>
<td align="right">-0.0100186</td>
</tr>
<tr class="odd">
<td>later</td>
<td align="right">-0.0097639</td>
</tr>
<tr class="even">
<td>girls</td>
<td align="right">-0.0096262</td>
</tr>
<tr class="odd">
<td>up</td>
<td align="right">-0.0090409</td>
</tr>
<tr class="even">
<td>course</td>
<td align="right">-0.0089834</td>
</tr>
<tr class="odd">
<td>just</td>
<td align="right">-0.0083408</td>
</tr>
<tr class="even">
<td>her</td>
<td align="right">-0.0082487</td>
</tr>
<tr class="odd">
<td>into</td>
<td align="right">-0.0081321</td>
</tr>
<tr class="even">
<td>out</td>
<td align="right">-0.0076929</td>
</tr>
<tr class="odd">
<td>at</td>
<td align="right">-0.0076502</td>
</tr>
<tr class="even">
<td>jimmy</td>
<td align="right">-0.0071043</td>
</tr>
<tr class="odd">
<td>when</td>
<td align="right">-0.0057891</td>
</tr>
<tr class="even">
<td>work</td>
<td align="right">-0.0054795</td>
</tr>
<tr class="odd">
<td>seems</td>
<td align="right">-0.0032164</td>
</tr>
<tr class="even">
<td>my</td>
<td align="right">-0.0030584</td>
</tr>
<tr class="odd">
<td>thing</td>
<td align="right">-0.0029429</td>
</tr>
<tr class="even">
<td>scene</td>
<td align="right">-0.0029061</td>
</tr>
<tr class="odd">
<td>all</td>
<td align="right">-0.0027010</td>
</tr>
<tr class="even">
<td>i</td>
<td align="right">-0.0024423</td>
</tr>
<tr class="odd">
<td>that</td>
<td align="right">-0.0015686</td>
</tr>
<tr class="even">
<td>altogether</td>
<td align="right">-0.0006508</td>
</tr>
<tr class="odd">
<td>opening</td>
<td align="right">0.0001563</td>
</tr>
<tr class="even">
<td>but</td>
<td align="right">0.0002473</td>
</tr>
<tr class="odd">
<td>amusing</td>
<td align="right">0.0003099</td>
</tr>
<tr class="even">
<td>songs</td>
<td align="right">0.0007721</td>
</tr>
<tr class="odd">
<td>fans</td>
<td align="right">0.0014369</td>
</tr>
<tr class="even">
<td>which</td>
<td align="right">0.0015447</td>
</tr>
<tr class="odd">
<td>she</td>
<td align="right">0.0022264</td>
</tr>
<tr class="even">
<td>begins</td>
<td align="right">0.0022827</td>
</tr>
<tr class="odd">
<td>really</td>
<td align="right">0.0028756</td>
</tr>
<tr class="even">
<td>past</td>
<td align="right">0.0029292</td>
</tr>
<tr class="odd">
<td>an</td>
<td align="right">0.0037487</td>
</tr>
<tr class="even">
<td>time</td>
<td align="right">0.0039243</td>
</tr>
<tr class="odd">
<td>more</td>
<td align="right">0.0044115</td>
</tr>
<tr class="even">
<td>are</td>
<td align="right">0.0047948</td>
</tr>
<tr class="odd">
<td>ex</td>
<td align="right">0.0049676</td>
</tr>
<tr class="even">
<td>they</td>
<td align="right">0.0055921</td>
</tr>
<tr class="odd">
<td>me</td>
<td align="right">0.0057067</td>
</tr>
<tr class="even">
<td>known</td>
<td align="right">0.0057068</td>
</tr>
<tr class="odd">
<td>than</td>
<td align="right">0.0059427</td>
</tr>
<tr class="even">
<td>not</td>
<td align="right">0.0059663</td>
</tr>
<tr class="odd">
<td>one</td>
<td align="right">0.0060378</td>
</tr>
<tr class="even">
<td>who</td>
<td align="right">0.0067409</td>
</tr>
<tr class="odd">
<td>other</td>
<td align="right">0.0076092</td>
</tr>
<tr class="even">
<td>it’s</td>
<td align="right">0.0077222</td>
</tr>
<tr class="odd">
<td>however</td>
<td align="right">0.0078614</td>
</tr>
<tr class="even">
<td>types</td>
<td align="right">0.0080658</td>
</tr>
<tr class="odd">
<td>for</td>
<td align="right">0.0081780</td>
</tr>
<tr class="even">
<td>same</td>
<td align="right">0.0082892</td>
</tr>
<tr class="odd">
<td>has</td>
<td align="right">0.0084059</td>
</tr>
<tr class="even">
<td>determine</td>
<td align="right">0.0084183</td>
</tr>
<tr class="odd">
<td>never</td>
<td align="right">0.0084232</td>
</tr>
<tr class="even">
<td>sense</td>
<td align="right">0.0084978</td>
</tr>
<tr class="odd">
<td>thief</td>
<td align="right">0.0094086</td>
</tr>
<tr class="even">
<td>it</td>
<td align="right">0.0105618</td>
</tr>
<tr class="odd">
<td>of</td>
<td align="right">0.0108933</td>
</tr>
<tr class="even">
<td>rather</td>
<td align="right">0.0113546</td>
</tr>
<tr class="odd">
<td>him</td>
<td align="right">0.0115767</td>
</tr>
<tr class="even">
<td>grows</td>
<td align="right">0.0122519</td>
</tr>
<tr class="odd">
<td>sequence</td>
<td align="right">0.0122732</td>
</tr>
<tr class="even">
<td>dollar</td>
<td align="right">0.0123269</td>
</tr>
<tr class="odd">
<td>love</td>
<td align="right">0.0130239</td>
</tr>
<tr class="even">
<td>a</td>
<td align="right">0.0142007</td>
</tr>
<tr class="odd">
<td>kind</td>
<td align="right">0.0150748</td>
</tr>
<tr class="even">
<td>use</td>
<td align="right">0.0150783</td>
</tr>
<tr class="odd">
<td>lives</td>
<td align="right">0.0152753</td>
</tr>
<tr class="even">
<td>you</td>
<td align="right">0.0153545</td>
</tr>
<tr class="odd">
<td>must</td>
<td align="right">0.0160841</td>
</tr>
<tr class="even">
<td>what</td>
<td align="right">0.0161985</td>
</tr>
<tr class="odd">
<td>while</td>
<td align="right">0.0163877</td>
</tr>
<tr class="even">
<td>will</td>
<td align="right">0.0164211</td>
</tr>
<tr class="odd">
<td>world</td>
<td align="right">0.0165024</td>
</tr>
<tr class="even">
<td>their</td>
<td align="right">0.0177427</td>
</tr>
<tr class="odd">
<td>feelings</td>
<td align="right">0.0178434</td>
</tr>
<tr class="even">
<td>word</td>
<td align="right">0.0180196</td>
</tr>
<tr class="odd">
<td>washington</td>
<td align="right">0.0198875</td>
</tr>
<tr class="even">
<td>grant</td>
<td align="right">0.0199556</td>
</tr>
<tr class="odd">
<td>by</td>
<td align="right">0.0208729</td>
</tr>
<tr class="even">
<td>parts</td>
<td align="right">0.0210454</td>
</tr>
<tr class="odd">
<td>prince</td>
<td align="right">0.0216244</td>
</tr>
<tr class="even">
<td>taking</td>
<td align="right">0.0221986</td>
</tr>
<tr class="odd">
<td>with</td>
<td align="right">0.0231662</td>
</tr>
<tr class="even">
<td>from</td>
<td align="right">0.0234281</td>
</tr>
<tr class="odd">
<td>probably</td>
<td align="right">0.0239042</td>
</tr>
<tr class="even">
<td>way</td>
<td align="right">0.0242771</td>
</tr>
<tr class="odd">
<td>i’ve</td>
<td align="right">0.0245634</td>
</tr>
<tr class="even">
<td>worked</td>
<td align="right">0.0249386</td>
</tr>
<tr class="odd">
<td>in</td>
<td align="right">0.0255450</td>
</tr>
<tr class="even">
<td>few</td>
<td align="right">0.0262726</td>
</tr>
<tr class="odd">
<td>done</td>
<td align="right">0.0263053</td>
</tr>
<tr class="even">
<td>the</td>
<td align="right">0.0287419</td>
</tr>
<tr class="odd">
<td>n</td>
<td align="right">0.0287966</td>
</tr>
<tr class="even">
<td>still</td>
<td align="right">0.0292254</td>
</tr>
<tr class="odd">
<td>biggest</td>
<td align="right">0.0296875</td>
</tr>
<tr class="even">
<td>is</td>
<td align="right">0.0310374</td>
</tr>
<tr class="odd">
<td>reality</td>
<td align="right">0.0313769</td>
</tr>
<tr class="even">
<td>keep</td>
<td align="right">0.0321016</td>
</tr>
<tr class="odd">
<td>good</td>
<td align="right">0.0340647</td>
</tr>
<tr class="even">
<td>film</td>
<td align="right">0.0344213</td>
</tr>
<tr class="odd">
<td>always</td>
<td align="right">0.0348183</td>
</tr>
<tr class="even">
<td>very</td>
<td align="right">0.0348716</td>
</tr>
<tr class="odd">
<td>and</td>
<td align="right">0.0370550</td>
</tr>
<tr class="even">
<td>moments</td>
<td align="right">0.0371737</td>
</tr>
<tr class="odd">
<td>between</td>
<td align="right">0.0372559</td>
</tr>
<tr class="even">
<td>though</td>
<td align="right">0.0389237</td>
</tr>
<tr class="odd">
<td>particularly</td>
<td align="right">0.0390302</td>
</tr>
<tr class="even">
<td>agree</td>
<td align="right">0.0441769</td>
</tr>
<tr class="odd">
<td>takes</td>
<td align="right">0.0448486</td>
</tr>
<tr class="even">
<td>able</td>
<td align="right">0.0480575</td>
</tr>
<tr class="odd">
<td>i’ll</td>
<td align="right">0.0480974</td>
</tr>
<tr class="even">
<td>eventually</td>
<td align="right">0.0484890</td>
</tr>
<tr class="odd">
<td>born</td>
<td align="right">0.0494355</td>
</tr>
<tr class="even">
<td>shot</td>
<td align="right">0.0500720</td>
</tr>
<tr class="odd">
<td>different</td>
<td align="right">0.0507236</td>
</tr>
<tr class="even">
<td>several</td>
<td align="right">0.0510954</td>
</tr>
<tr class="odd">
<td>he</td>
<td align="right">0.0522909</td>
</tr>
<tr class="even">
<td>killers</td>
<td align="right">0.0530381</td>
</tr>
<tr class="odd">
<td>clear</td>
<td align="right">0.0549046</td>
</tr>
<tr class="even">
<td>attention</td>
<td align="right">0.0549228</td>
</tr>
<tr class="odd">
<td>contact</td>
<td align="right">0.0574982</td>
</tr>
<tr class="even">
<td>as</td>
<td align="right">0.0626043</td>
</tr>
<tr class="odd">
<td>multi</td>
<td align="right">0.0640192</td>
</tr>
<tr class="even">
<td>deal</td>
<td align="right">0.0648396</td>
</tr>
<tr class="odd">
<td>also</td>
<td align="right">0.0659141</td>
</tr>
<tr class="even">
<td>become</td>
<td align="right">0.0692140</td>
</tr>
<tr class="odd">
<td>fairly</td>
<td align="right">0.0745365</td>
</tr>
<tr class="even">
<td>narrative</td>
<td align="right">0.0791347</td>
</tr>
<tr class="odd">
<td>performances</td>
<td align="right">0.0816109</td>
</tr>
<tr class="even">
<td>music</td>
<td align="right">0.0820430</td>
</tr>
<tr class="odd">
<td>neighbor</td>
<td align="right">0.0848025</td>
</tr>
<tr class="even">
<td>watches</td>
<td align="right">0.0876229</td>
</tr>
<tr class="odd">
<td>broke</td>
<td align="right">0.0936433</td>
</tr>
<tr class="even">
<td>natural</td>
<td align="right">0.0969546</td>
</tr>
<tr class="odd">
<td>reminiscent</td>
<td align="right">0.1004395</td>
</tr>
<tr class="even">
<td>voices</td>
<td align="right">0.1151647</td>
</tr>
<tr class="odd">
<td>excellent</td>
<td align="right">0.1228298</td>
</tr>
<tr class="even">
<td>intense</td>
<td align="right">0.1262374</td>
</tr>
<tr class="odd">
<td>frightening</td>
<td align="right">0.1264323</td>
</tr>
<tr class="even">
<td>oliver</td>
<td align="right">0.1275805</td>
</tr>
<tr class="odd">
<td>job</td>
<td align="right">0.1327543</td>
</tr>
<tr class="even">
<td>husband</td>
<td align="right">0.1379187</td>
</tr>
<tr class="odd">
<td>distracting</td>
<td align="right">0.1526334</td>
</tr>
<tr class="even">
<td>soundtrack</td>
<td align="right">0.1612572</td>
</tr>
<tr class="odd">
<td>industry</td>
<td align="right">0.2074668</td>
</tr>
<tr class="even">
<td>6</td>
<td align="right">0.4226458</td>
</tr>
</tbody>
</table>
<p>Y notamos que en primer caso, palabras “cell” es consideradas
como positivas y en el segundo caso, se considera la ocurrencia de “6”
como positiva. Sin embargo, observamos que en ambos casos la
palabra problemática es usada de manera distinta en estas reseñas
que en el resto (la primera es la película The Cell, y la segunda es
Girl 6). La frecuencia alta de estas palabras en estas dos reseñas
contribuye incorrectamente a denotar estas reseñas como positivas.</p>
<p>Estas películas <em>extrapolan</em> demasiado lejos de los datos de entrenamiento.
Típicamente, la extrapolación fuerte produce problemas tanto de
<strong>sesgo</strong> (modelo poco apropiado para los valores que observamos)
como <strong>varianza</strong>, pues estamos haciendo predicciones donde hay
pocos datos.</p>
<p>En este caso, podemos intentar reducir el sesgo tomando el logaritmo
de los conteos de palabras en lugar de los conteos crudos. Esto reduce
la influencia de conteos altos de palabras en relación a conteos altos
(también podemos intentar usar indicadoras 0-1 en lugar de conteos).
Después de algunos experimentos, podemos mejorar un poco:</p>
<pre class="sourceCode r"><code class="sourceCode r">usar_cache &lt;-<span class="st"> </span><span class="ot">TRUE</span>
vocabulario &lt;-<span class="st"> </span><span class="kw">calc_vocabulario</span>(df_ent_grande, <span class="dv">8500</span>, <span class="dt">remove_stop =</span> <span class="ot">FALSE</span>)
<span class="cf">if</span>(<span class="op">!</span>usar_cache){
    mod_x &lt;-<span class="st"> </span><span class="kw">correr_modelo_cv</span>(df_ent_grande, df_pr, vocabulario, 
                              <span class="dt">alpha =</span> <span class="fl">0.01</span>, <span class="dt">lambda =</span> <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>, <span class="fl">0.1</span>)),
                              <span class="dt">log_transform =</span> <span class="ot">TRUE</span>)
    <span class="kw">saveRDS</span>(mod_x, <span class="dt">file =</span> <span class="st">&quot;./cache_obj/mod_sentiment_log.rds&quot;</span>)
  } <span class="cf">else</span> {
    mod_x &lt;-<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;./cache_obj/mod_sentiment_log.rds&quot;</span>)
  }
<span class="kw">describir_modelo_cv</span>(mod_x)</code></pre>
<p><img src="06-diag-mejora_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<pre><code>## [1] &quot;Lambda min: 0.0273237224472926&quot;
## [1] &quot;Error entrenamiento: 0&quot;
## [1] &quot;Error prueba: 0.11&quot;
## [1] &quot;Devianza entrena:0.045&quot;
## [1] &quot;Devianza prueba:0.565&quot;</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="descenso-estocastico.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="metodos-basados-en-arboles.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/felipegonzalez/aprendizaje-maquina-verano-2018/edit/master/06-diag-mejora.Rmd",
"text": "Edit"
},
"download": ["am-curso-verano.pdf", "am-curso-verano.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
